{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e4946738-b2f4-498b-b3e1-5232b71a9504",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tue Mar 14 15:21:37 2023       \n",
      "+-----------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 510.108.03   Driver Version: 510.108.03   CUDA Version: 11.6     |\n",
      "|-------------------------------+----------------------+----------------------+\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                               |                      |               MIG M. |\n",
      "|===============================+======================+======================|\n",
      "|   0  NVIDIA GeForce ...  Off  | 00000000:1B:00.0 Off |                  Off |\n",
      "|  0%   43C    P8    25W / 460W |      5MiB / 24564MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   1  NVIDIA GeForce ...  Off  | 00000000:1C:00.0 Off |                  N/A |\n",
      "| 27%   35C    P8    15W / 260W |   4183MiB / 11264MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   2  NVIDIA GeForce ...  Off  | 00000000:1E:00.0 Off |                  N/A |\n",
      "| 27%   35C    P8    12W / 260W |      5MiB / 11264MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   3  NVIDIA GeForce ...  Off  | 00000000:3F:00.0 Off |                  Off |\n",
      "|  0%   32C    P8    19W / 460W |      5MiB / 24564MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   4  NVIDIA GeForce ...  Off  | 00000000:40:00.0 Off |                  Off |\n",
      "|  0%   37C    P8    28W / 460W |      5MiB / 24564MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   5  NVIDIA GeForce ...  Off  | 00000000:5E:00.0 Off |                  Off |\n",
      "|  0%   36C    P8    25W / 460W |      5MiB / 24564MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "                                                                               \n",
      "+-----------------------------------------------------------------------------+\n",
      "| Processes:                                                                  |\n",
      "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
      "|        ID   ID                                                   Usage      |\n",
      "|=============================================================================|\n",
      "+-----------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e167a3bc-5f8b-4e15-8c61-d427dfdcac4b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting torch\n",
      "  Downloading torch-1.13.1-cp310-cp310-manylinux1_x86_64.whl (887.5 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m887.5/887.5 MB\u001b[0m \u001b[31m3.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting nvidia-cuda-nvrtc-cu11==11.7.99\n",
      "  Downloading nvidia_cuda_nvrtc_cu11-11.7.99-2-py3-none-manylinux1_x86_64.whl (21.0 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.0/21.0 MB\u001b[0m \u001b[31m30.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: typing-extensions in /opt/conda/lib/python3.10/site-packages (from torch) (4.4.0)\n",
      "Collecting nvidia-cublas-cu11==11.10.3.66\n",
      "  Downloading nvidia_cublas_cu11-11.10.3.66-py3-none-manylinux1_x86_64.whl (317.1 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m317.1/317.1 MB\u001b[0m \u001b[31m8.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting nvidia-cudnn-cu11==8.5.0.96\n",
      "  Downloading nvidia_cudnn_cu11-8.5.0.96-2-py3-none-manylinux1_x86_64.whl (557.1 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m557.1/557.1 MB\u001b[0m \u001b[31m5.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting nvidia-cuda-runtime-cu11==11.7.99\n",
      "  Downloading nvidia_cuda_runtime_cu11-11.7.99-py3-none-manylinux1_x86_64.whl (849 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m849.3/849.3 kB\u001b[0m \u001b[31m62.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: setuptools in /opt/conda/lib/python3.10/site-packages (from nvidia-cublas-cu11==11.10.3.66->torch) (66.1.1)\n",
      "Requirement already satisfied: wheel in /opt/conda/lib/python3.10/site-packages (from nvidia-cublas-cu11==11.10.3.66->torch) (0.38.4)\n",
      "Installing collected packages: nvidia-cuda-runtime-cu11, nvidia-cuda-nvrtc-cu11, nvidia-cublas-cu11, nvidia-cudnn-cu11, torch\n",
      "Successfully installed nvidia-cublas-cu11-11.10.3.66 nvidia-cuda-nvrtc-cu11-11.7.99 nvidia-cuda-runtime-cu11-11.7.99 nvidia-cudnn-cu11-8.5.0.96 torch-1.13.1\n",
      "Collecting transformers\n",
      "  Downloading transformers-4.26.1-py3-none-any.whl (6.3 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.3/6.3 MB\u001b[0m \u001b[31m17.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.10/site-packages (from transformers) (6.0)\n",
      "Requirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.10/site-packages (from transformers) (1.23.5)\n",
      "Requirement already satisfied: tqdm>=4.27 in /opt/conda/lib/python3.10/site-packages (from transformers) (4.64.1)\n",
      "Collecting tokenizers!=0.11.3,<0.14,>=0.11.1\n",
      "  Downloading tokenizers-0.13.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.6 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.6/7.6 MB\u001b[0m \u001b[31m40.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0mm\n",
      "\u001b[?25hCollecting filelock\n",
      "  Downloading filelock-3.9.1-py3-none-any.whl (9.7 kB)\n",
      "Requirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from transformers) (2.28.2)\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.10/site-packages (from transformers) (23.0)\n",
      "Collecting huggingface-hub<1.0,>=0.11.0\n",
      "  Downloading huggingface_hub-0.13.2-py3-none-any.whl (199 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m199.2/199.2 kB\u001b[0m \u001b[31m58.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting regex!=2019.12.17\n",
      "  Downloading regex-2022.10.31-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (770 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m770.5/770.5 kB\u001b[0m \u001b[31m74.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.11.0->transformers) (4.4.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (3.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (2022.12.7)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (1.26.14)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (2.1.1)\n",
      "Installing collected packages: tokenizers, regex, filelock, huggingface-hub, transformers\n",
      "Successfully installed filelock-3.9.1 huggingface-hub-0.13.2 regex-2022.10.31 tokenizers-0.13.2 transformers-4.26.1\n"
     ]
    }
   ],
   "source": [
    "!pip install torch\n",
    "!pip install transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8c6b35a9-023c-4ff8-8739-bc978c1a4be0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "from transformers import T5ForConditionalGeneration, AutoTokenizer\n",
    "from transformers import AutoModelForSeq2SeqLM, AutoTokenizer, AutoConfig\n",
    "\n",
    "\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import Trainer, TrainingArguments\n",
    "from transformers.file_utils import cached_property\n",
    "from typing import Tuple\n",
    "from sklearn.model_selection import train_test_split\n",
    "import gc\n",
    "from tqdm.auto import tqdm, trange\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "from sklearn.utils import shuffle\n",
    "\n",
    "from collections import Counter\n",
    "from tqdm import tqdm\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9a913f3-d530-4177-b1a6-97a87397db80",
   "metadata": {},
   "source": [
    "# model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d3f49cbf-db6e-4ce6-addd-d1301fd93a57",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tue Mar 14 15:23:09 2023       \n",
      "+-----------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 510.108.03   Driver Version: 510.108.03   CUDA Version: 11.6     |\n",
      "|-------------------------------+----------------------+----------------------+\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                               |                      |               MIG M. |\n",
      "|===============================+======================+======================|\n",
      "|   0  NVIDIA GeForce ...  Off  | 00000000:1B:00.0 Off |                  Off |\n",
      "|  0%   41C    P8    25W / 460W |      5MiB / 24564MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   1  NVIDIA GeForce ...  Off  | 00000000:1C:00.0 Off |                  N/A |\n",
      "| 27%   34C    P8    14W / 260W |   4183MiB / 11264MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   2  NVIDIA GeForce ...  Off  | 00000000:1E:00.0 Off |                  N/A |\n",
      "| 27%   35C    P8    11W / 260W |      5MiB / 11264MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   3  NVIDIA GeForce ...  Off  | 00000000:3F:00.0 Off |                  Off |\n",
      "|  0%   32C    P8    19W / 460W |      5MiB / 24564MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   4  NVIDIA GeForce ...  Off  | 00000000:40:00.0 Off |                  Off |\n",
      "|  0%   37C    P8    29W / 460W |      5MiB / 24564MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   5  NVIDIA GeForce ...  Off  | 00000000:5E:00.0 Off |                  Off |\n",
      "|  0%   36C    P8    25W / 460W |      5MiB / 24564MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "                                                                               \n",
      "+-----------------------------------------------------------------------------+\n",
      "| Processes:                                                                  |\n",
      "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
      "|        ID   ID                                                   Usage      |\n",
      "|=============================================================================|\n",
      "+-----------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ba76c0c3-a3c0-40ea-b5f4-1b7a8ad9ac8b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os \n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]='0'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8a26c278-e3ee-4084-97f6-dac02ab584b9",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU\n"
     ]
    }
   ],
   "source": [
    "if torch.cuda.is_available():\n",
    "    device='cuda'\n",
    "    print('GPU')\n",
    "else:\n",
    "    device='cpu'\n",
    "    print('CPU')\n",
    "    \n",
    "    \n",
    "SEED = 0\n",
    "torch.manual_seed(SEED)\n",
    "torch.cuda.manual_seed(SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ec73cecd-abb8-4635-a184-430959e38258",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0f3bd1551e774e948b6e97402d0084cd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)lve/main/config.json:   0%|          | 0.00/1.21k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "88c0dee3df964a7987ac9b929a2333ce",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading pytorch_model.bin:   0%|          | 0.00/2.95G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "84c52510cf394d49adbe4e6adc094c23",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)neration_config.json:   0%|          | 0.00/147 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3b67fa36cee24e17919e8a3f88c75831",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)ve/main/spiece.model:   0%|          | 0.00/792k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f924b707ae78447dbafb364265878f05",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)/main/tokenizer.json:   0%|          | 0.00/1.39M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/transformers/models/t5/tokenization_t5_fast.py:155: FutureWarning: This tokenizer was incorrectly instantiated with a model max length of 512 which will be corrected in Transformers v5.\n",
      "For now, this behavior is kept to avoid breaking backwards compatibility when padding/encoding with `truncation is True`.\n",
      "- Be aware that you SHOULD NOT rely on t5-large automatically truncating your input to 512 when padding/encoding.\n",
      "- If you want to encode/pad to sequences longer than 512 you can either instantiate this tokenizer with `model_max_length` or pass `max_length` when encoding/padding.\n",
      "- To avoid this warning, please instantiate this tokenizer with `model_max_length` set to your preferred value.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from transformers import TrainingArguments, Trainer\n",
    "\n",
    "model_checkpoint = \"t5-large\"\n",
    "model = T5ForConditionalGeneration.from_pretrained(model_checkpoint).to(device)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7ef1229c-d458-413c-acc9-2fb0221248b9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def cleanup():\n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()\n",
    "    \n",
    "cleanup()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6c063e4-fa59-487c-96a0-d9a8cf7f35f5",
   "metadata": {},
   "source": [
    "# **Read data SemEval2018-Task9**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9ded36d6-134b-412d-9842-6fedeb6a9ed0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "path = os.getcwd().replace('hearst_patterns', '')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c2344944-6187-49b1-bcb9-0f1b98c505ce",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>term</th>\n",
       "      <th>relation</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>blackfly</td>\n",
       "      <td>Concept</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Turonian</td>\n",
       "      <td>Entity</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>abhorrence</td>\n",
       "      <td>Concept</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>tropical storm</td>\n",
       "      <td>Concept</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>militarization</td>\n",
       "      <td>Concept</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             term relation\n",
       "0        blackfly  Concept\n",
       "1        Turonian   Entity\n",
       "2      abhorrence  Concept\n",
       "3  tropical storm  Concept\n",
       "4  militarization  Concept"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "path_data_en = path+\"SemEval2018-Task9/training/data/1A.english.training.data.txt\"\n",
    "path_gold_en = path+\"SemEval2018-Task9/training/gold/1A.english.training.gold.txt\"\n",
    "\n",
    "train_data_en_data = pd.read_csv(path_data_en, header=None, sep=\"\\t\", names=['term', 'relation'])\n",
    "train_gold_en_data = pd.read_csv(path_gold_en, header=None, names=['hypernym'])\n",
    "\n",
    "train_data_en_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "129edda7-bf42-43e4-9fd0-7e55aa8bc714",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "path_test_data_en = path+\"SemEval2018-Task9/test/data/1A.english.test.data.txt\"\n",
    "path_test_gold_en = path+\"SemEval2018-Task9/test/gold/1A.english.test.gold.txt\"\n",
    "\n",
    "test_data_en_data = pd.read_csv(path_test_data_en, header=None, sep=\"\\t\", names=['term', 'relation'])\n",
    "test_gold_en_data = pd.read_csv(path_test_gold_en, header=None, names=['hypernym'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "37744552-c78d-4308-8c36-a256835839ea",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['There are a lot of [PARENT] such as',\n",
       " 'There were a lot of [PARENT] such as',\n",
       " 'There are a lot of [PARENT] here such as',\n",
       " 'Other [PARENT] such as',\n",
       " 'My favorite [PARENT] is either',\n",
       " 'There were a lot of [PARENT] here such as',\n",
       " 'which includes various [PARENT] like',\n",
       " 'Other [PARENT] especially',\n",
       " 'which includes various [PARENT] such as',\n",
       " 'My favorite [PARENT] is',\n",
       " 'I know such types of [PARENT] as',\n",
       " 'I know such kinds of [PARENT] as',\n",
       " '[PARENT] such as',\n",
       " 'I know many kinds of [PARENT] for example',\n",
       " 'Other [PARENT] for example',\n",
       " '[PARENT] namely',\n",
       " 'I know many types of [PARENT] for example',\n",
       " '[PARENT] including',\n",
       " 'There are a lot of [PARENT] for example',\n",
       " 'which includes various [PARENT] for example',\n",
       " 'There are a lot of [PARENT] here for example',\n",
       " '[PARENT] e.g.',\n",
       " '[PARENT] like',\n",
       " '[PARENT] especially',\n",
       " '[PARENT] for example',\n",
       " '[PARENT] for instance']"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hearst_patterns = \"\"\"\n",
    "There are a lot of [PARENT] such as\n",
    "\n",
    "There were a lot of [PARENT] such as\n",
    "\n",
    "There are a lot of [PARENT] here such as\n",
    "\n",
    "Other [PARENT] such as\n",
    "\n",
    "My favorite [PARENT] is either\n",
    "\n",
    "There were a lot of [PARENT] here such as\n",
    "\n",
    "which includes various [PARENT] like\n",
    "\n",
    "Other [PARENT] especially\n",
    "\n",
    "which includes various [PARENT] such as\n",
    "\n",
    "My favorite [PARENT] is\n",
    "\n",
    "I know such types of [PARENT] as\n",
    "\n",
    "I know such kinds of [PARENT] as\n",
    "\n",
    "[PARENT] such as\n",
    "\n",
    "I know many kinds of [PARENT] for example\n",
    "\n",
    "Other [PARENT] for example\n",
    "\n",
    "[PARENT] namely\n",
    "\n",
    "I know many types of [PARENT] for example\n",
    "\n",
    "[PARENT] including\n",
    "\n",
    "There are a lot of [PARENT] for example\n",
    "\n",
    "which includes various [PARENT] for example\n",
    "\n",
    "There are a lot of [PARENT] here for example\n",
    "\n",
    "[PARENT] e.g.\n",
    "\n",
    "[PARENT] like\n",
    "\n",
    "[PARENT] especially\n",
    "\n",
    "[PARENT] for example\n",
    "\n",
    "[PARENT] for instance\n",
    "\"\"\"\n",
    "\n",
    "hearst_patterns = [pattern for ind, pattern in enumerate(hearst_patterns.split('\\n')) if ind % 2 != 0]  \n",
    "hearst_patterns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "c626a99f-dcc7-44d3-b1cc-1150671d9133",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def hearest_preprocessing(train_features, train_target, test_features, test_target, hearst_pattern):\n",
    "    hearst_pattern = hearst_pattern.replace('[PARENT]', '<extra_id_0>')\n",
    "    \n",
    "    prefix = 'fill | '\n",
    "    prefix=''\n",
    "        \n",
    "    train_data_en = train_features.copy()\n",
    "    train_data_en = prefix + hearst_pattern + ' ' + train_data_en_data.term +  ' </s>'\n",
    "    print(train_data_en.head())\n",
    "\n",
    "    train_gold_en = train_target.copy()\n",
    "    train_gold_en = train_gold_en.hypernym.str.split('\\t').str.join(', ')\n",
    "    print(train_gold_en.head())\n",
    "    \n",
    "    test_data_en = test_features.copy()\n",
    "    test_data_en = prefix + hearst_pattern + ' ' + test_data_en.term +  ' </s>'\n",
    "    print(test_data_en.head())\n",
    "\n",
    "    test_gold_en = test_target.copy()\n",
    "    test_gold_en = test_gold_en.hypernym.str.split('\\t').str.join(', ')\n",
    "    print(test_gold_en.head())\n",
    "    \n",
    "    return train_data_en, train_gold_en, test_data_en, test_gold_en\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "104ea460-4fcd-44c6-85f6-44af748bc722",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    There are a lot of <extra_id_0> such as blackf...\n",
      "1    There are a lot of <extra_id_0> such as Turoni...\n",
      "2    There are a lot of <extra_id_0> such as abhorr...\n",
      "3    There are a lot of <extra_id_0> such as tropic...\n",
      "4    There are a lot of <extra_id_0> such as milita...\n",
      "Name: term, dtype: object\n",
      "0                           homopterous insect, insect\n",
      "1    technical specification, geologic timescale, p...\n",
      "2                      distaste, hatred, hate, disgust\n",
      "3    atmosphere, windstorm, violent storm, air curr...\n",
      "4                                       social control\n",
      "Name: hypernym, dtype: object\n",
      "0    There are a lot of <extra_id_0> such as malici...\n",
      "1    There are a lot of <extra_id_0> such as buckle...\n",
      "2    There are a lot of <extra_id_0> such as spelun...\n",
      "3    There are a lot of <extra_id_0> such as quo wa...\n",
      "4    There are a lot of <extra_id_0> such as Jeff F...\n",
      "Name: term, dtype: object\n",
      "0       malevolence, distaste, hatred, hate, malignity\n",
      "1                                           body armor\n",
      "2                    exploration, adventurer, explorer\n",
      "3    proceedings, legal proceedings, proceeding, du...\n",
      "4               thrower, baseball player, jock, person\n",
      "Name: hypernym, dtype: object\n"
     ]
    }
   ],
   "source": [
    "train_data_en, train_gold_en, test_data_en, test_gold_en = hearest_preprocessing(train_data_en_data, \n",
    "                                                                                 train_gold_en_data, \n",
    "                                                                                 test_data_en_data, \n",
    "                                                                                 test_gold_en_data, \n",
    "                                                                                 hearst_patterns[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b52fad45-8557-4569-85e9-84a9c8de4d1c",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "18b98665-8190-48db-9128-eaf597e80615",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# use only 1 time per session\n",
    "model_checkpoint = \"t5-large\"\n",
    "model = T5ForConditionalGeneration.from_pretrained(model_checkpoint).to(device)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_checkpoint, max_length=100, block_size=64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "a0719ff4-5ce2-47b8-ad6e-6fa006c96d22",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['India is a part of the world. </s>',\n",
       " 'India is a part of the world. </s>',\n",
       " 'India is a different part of the world. </s>',\n",
       " 'India is a small part of the world. </s>',\n",
       " 'India is a melting pot of the world. </s>',\n",
       " 'India is a big part of the world. </s>',\n",
       " 'India is a very different part of the world. </s>',\n",
       " 'India is a corner of the world. </s>',\n",
       " 'India is a unique part of the world. </s>',\n",
       " 'India is a beautiful part of the world. </s>',\n",
       " 'India is a part of the world. </s>',\n",
       " 'India is a huge part of the world. </s>',\n",
       " 'India is a new part of the world. </s>',\n",
       " 'India is a great part of the world. </s>',\n",
       " 'India is a vibrant part of the world. </s>',\n",
       " 'India is a vast part of the world. </s>',\n",
       " 'India is a diverse part of the world. </s>',\n",
       " 'India is a part of the world. </s>',\n",
       " 'India is a large part of the world. </s>',\n",
       " 'India is a powerhouse of the world. </s>']"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Input text\n",
    "text = 'India is a <extra_id_0> of the world. </s>'\n",
    "\n",
    "encoded = tokenizer.encode_plus(text, add_special_tokens=True, return_tensors='pt')\n",
    "input_ids = encoded['input_ids'].to(device)\n",
    "\n",
    "# Generaing 20 sequences with maximum length set to 5\n",
    "outputs = model.generate(input_ids=input_ids, \n",
    "                          num_beams=200, num_return_sequences=20,\n",
    "                          max_length=5)\n",
    "\n",
    "_0_index = text.index('<extra_id_0>')\n",
    "_result_prefix = text[:_0_index]\n",
    "_result_suffix = text[_0_index+12:]  # 12 is the length of <extra_id_0>\n",
    "\n",
    "def _filter(output, end_token='<extra_id_1>'):\n",
    "    # The first token is <unk> (inidex at 0) and the second token is <extra_id_0> (indexed at 32099)\n",
    "    _txt = tokenizer.decode(output[2:], skip_special_tokens=False, clean_up_tokenization_spaces=False)\n",
    "    if end_token in _txt:\n",
    "        _end_token_index = _txt.index(end_token)\n",
    "        return _result_prefix + _txt[:_end_token_index] + _result_suffix\n",
    "    else:\n",
    "        return _result_prefix + _txt + _result_suffix\n",
    "\n",
    "results = list(map(_filter, outputs))\n",
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "d1d24206-3655-468e-b961-964bde0bc939",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pattern There are a lot of <extra_id_0> such as maliciousness </s>\n",
      "output ['reasons', 'reasons for this', 'reasons', 'reasons for this', 'reasons', 'reasons', 'reasons for it', 'reasons', 'factors', 'reasons', 'reasons', 'reasons', 'reasons', 'reasons', 'other reasons', 'reasons', 'dangers', 'reasons', 'factors', 'factors']\n",
      "\n",
      "pattern There are a lot of <extra_id_0> such as buckler </s>\n",
      "output ['buckles', 'bucklers', 'buckles', 'belt buckles', 'buckles', 'accessories', 'buckler', 'different buckles', 'buckle accessories', 'buckles', 'options', 'options', 'buckle designs', 'accessories', 'accessories', 'options', 'options', 'buckle accessories', 'accessories', 'belt buckle']\n",
      "\n",
      "pattern There are a lot of <extra_id_0> such as spelunker </s>\n",
      "output ['online games', 'games', 'websites', 'games', 'sites', 'search engines', 'them', 'different games', 'games to play', 'games online', 'other games', 'games', 'fun games', 'games', 'websites', 'popular games', 'games available', 'games', 'sites', 'them']\n",
      "\n",
      "pattern There are a lot of <extra_id_0> such as quo warranto </s>\n",
      "output ['legal terms', 'different types', 'examples', 'legal issues', 'ways to do this', 'examples', 'different forms', 'ways to do this', 'ways to do it', 'ways to do this', 'different kinds', 'ways to do it', 'options', 'different types of cases', 'cases', 'ways', 'different kinds of cases', 'ways to do that', 'ways to do it', 'variations']\n",
      "\n",
      "pattern There are a lot of <extra_id_0> such as Jeff Francis </s>\n",
      "output ['people', 'celebrities', 'artists', 'great artists', 'people', 'writers', 'talented artists', 'authors', 'great writers', 'celebrities', 'great authors', 'artists', 'great musicians', 'talented musicians', 'talented people', 'musicians', 'experts', 'actors', 'professionals', 'people']\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "regex = \"[a-zA-Z]+\"\n",
    "\n",
    "\n",
    "def _filter(output, end_token='<extra_id_1>'):\n",
    "        # The first token is <unk> (inidex at 0) and the second token is <extra_id_0> (indexed at 32099)\n",
    "        _txt = tokenizer.decode(output[2:], skip_special_tokens=False, clean_up_tokenization_spaces=False)\n",
    "        if end_token in _txt:\n",
    "            _end_token_index = _txt.index(end_token)\n",
    "            return _txt[:_end_token_index]\n",
    "        else:\n",
    "            return _txt\n",
    "\n",
    "        \n",
    "def predict_token(text):\n",
    "    \n",
    "    encoded = tokenizer.encode_plus(text, add_special_tokens=False, return_tensors='pt')\n",
    "    input_ids = encoded['input_ids'].to(device)\n",
    "\n",
    "    # Generaing 20 sequences with maximum length set to 10\n",
    "    outputs = model.generate(input_ids=input_ids, \n",
    "                              num_beams=20, num_return_sequences=20,\n",
    "                              max_length=10)\n",
    "\n",
    "    _0_index = text.index('<extra_id_0>')\n",
    "    _result_prefix = text[:_0_index]\n",
    "    _result_suffix = text[_0_index+12:]  # 12 is the length of <extra_id_0>\n",
    "    \n",
    "    results = list(map(_filter, outputs))\n",
    "    results = [test_string.replace(',' , '')for test_string in results]\n",
    "    \n",
    "    return results\n",
    "\n",
    "for i in range(5):\n",
    "    text = test_data_en.tolist()[i]\n",
    "    print('pattern', text)\n",
    "    results = predict_token(text)\n",
    "    \n",
    "    print('output', results)\n",
    "    print()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eaec1fb2-d985-4533-9bf0-e6eb06f31f4f",
   "metadata": {
    "tags": []
   },
   "source": [
    "# EVALUATION "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "94edd7e6-385b-4747-99a1-b98bba45590b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def predict(test_data_en, test_gold_en):\n",
    "    \n",
    "#   make predictions for each hyponyms\n",
    "    test_pred_en=[]\n",
    "    for text in tqdm(test_data_en.tolist()):\n",
    "        pred_masked_token = predict_token(text)\n",
    "        test_pred_en.append('\\t'.join(pred_masked_token))\n",
    "            \n",
    "#   make txt format\n",
    "    name  = 'pred_hearst_pattern.txt'\n",
    "\n",
    "    test_pred_en_df = pd.DataFrame(test_pred_en)\n",
    "    test_pred_en_df.to_csv(name, header=None, index=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "71a04b19-a666-4b10-a6e4-d194ed81e261",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1500/1500 [09:23<00:00,  2.66it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 9min 22s, sys: 982 ms, total: 9min 23s\n",
      "Wall time: 9min 23s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "%%time \n",
    "\n",
    "predict(test_data_en, test_gold_en)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a0d0537b-b9c1-452d-a98d-c24e46a69228",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from contextlib import redirect_stdout\n",
    "import io"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1396bcf7-12a2-4e4d-b520-1f26edbbcbd7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def answers(str_ans, name_of_pattern):\n",
    "    str_ans += name_of_pattern\n",
    "    columns_name = []\n",
    "    values = []\n",
    "    for ind, metrics in enumerate(_std_out.split('\\r\\n')):\n",
    "        \n",
    "        if ind == 6:\n",
    "            _name = 'name of hearst pattern'\n",
    "            number = name_of_pattern\n",
    "        else:\n",
    "            _name, number = metrics.split(' ')\n",
    "            number = round(float(number), 5)\n",
    "            _name = _name[:-1]\n",
    "        \n",
    "        columns_name.append(_name)\n",
    "        values.append([number])\n",
    "        \n",
    "    \n",
    "        \n",
    "    df = pd.DataFrame(values).T\n",
    "    df.columns = columns_name\n",
    "    df.set_index('name of hearst pattern', inplace=True)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "03be1639-0415-4dd9-825f-7b3e800b7117",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>MRR</th>\n",
       "      <th>MAP</th>\n",
       "      <th>P@1</th>\n",
       "      <th>P@3</th>\n",
       "      <th>P@5</th>\n",
       "      <th>P@15</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>name of hearst pattern</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>There are a lot of [PARENT] such as</th>\n",
       "      <td>0.05528</td>\n",
       "      <td>0.02892</td>\n",
       "      <td>0.04133</td>\n",
       "      <td>0.028</td>\n",
       "      <td>0.02796</td>\n",
       "      <td>0.02821</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                         MRR      MAP      P@1    P@3  \\\n",
       "name of hearst pattern                                                  \n",
       "There are a lot of [PARENT] such as  0.05528  0.02892  0.04133  0.028   \n",
       "\n",
       "                                         P@5     P@15  \n",
       "name of hearst pattern                                 \n",
       "There are a lot of [PARENT] such as  0.02796  0.02821  "
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "name_of_pattern = hearst_patterns[0]\n",
    "f = io.StringIO()\n",
    "with redirect_stdout(f):\n",
    "    !python scorer.py /home/jovyan/work/SemEval2018-Task9/test/gold/1A.english.test.gold.txt pred_hearst_pattern.txt\n",
    "\n",
    "_std_out = f.getvalue()\n",
    "\n",
    "\n",
    "_std_out = _std_out  + name_of_pattern\n",
    "output = answers(_std_out, name_of_pattern)\n",
    "output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "634bfee4-080a-4111-8a7c-a6bc1ec19ec6",
   "metadata": {
    "tags": []
   },
   "source": [
    "# All hearst patterns table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "392962da-1018-4a4c-bdf0-d6a86c700f55",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "if 'hearst_patterns_result_table.csv' in os.listdir():\n",
    "    df = pd.read_csv('hearst_patterns_result_table.csv')\n",
    "    last_pattern = df['name of hearst pattern'].tail(1).values[0]\n",
    "    \n",
    "    ind = hearst_patterns.index(last_pattern)\n",
    "    hearst_patterns_in_use = hearst_patterns[ind+1:]\n",
    "    \n",
    "else:\n",
    "    hearst_patterns_in_use = hearst_patterns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "5f3d08a6-0870-4a60-9d5a-297b56f660c7",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    There are a lot of <extra_id_0> here such as b...\n",
      "1    There are a lot of <extra_id_0> here such as T...\n",
      "2    There are a lot of <extra_id_0> here such as a...\n",
      "3    There are a lot of <extra_id_0> here such as t...\n",
      "4    There are a lot of <extra_id_0> here such as m...\n",
      "Name: term, dtype: object\n",
      "0                           homopterous insect, insect\n",
      "1    technical specification, geologic timescale, p...\n",
      "2                      distaste, hatred, hate, disgust\n",
      "3    atmosphere, windstorm, violent storm, air curr...\n",
      "4                                       social control\n",
      "Name: hypernym, dtype: object\n",
      "0    There are a lot of <extra_id_0> here such as m...\n",
      "1    There are a lot of <extra_id_0> here such as b...\n",
      "2    There are a lot of <extra_id_0> here such as s...\n",
      "3    There are a lot of <extra_id_0> here such as q...\n",
      "4    There are a lot of <extra_id_0> here such as J...\n",
      "Name: term, dtype: object\n",
      "0       malevolence, distaste, hatred, hate, malignity\n",
      "1                                           body armor\n",
      "2                    exploration, adventurer, explorer\n",
      "3    proceedings, legal proceedings, proceeding, du...\n",
      "4               thrower, baseball player, jock, person\n",
      "Name: hypernym, dtype: object\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1500/1500 [08:58<00:00,  2.79it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "0          Other <extra_id_0> such as blackfly </s>\n",
      "1          Other <extra_id_0> such as Turonian </s>\n",
      "2        Other <extra_id_0> such as abhorrence </s>\n",
      "3    Other <extra_id_0> such as tropical storm </s>\n",
      "4    Other <extra_id_0> such as militarization </s>\n",
      "Name: term, dtype: object\n",
      "0                           homopterous insect, insect\n",
      "1    technical specification, geologic timescale, p...\n",
      "2                      distaste, hatred, hate, disgust\n",
      "3    atmosphere, windstorm, violent storm, air curr...\n",
      "4                                       social control\n",
      "Name: hypernym, dtype: object\n",
      "0    Other <extra_id_0> such as maliciousness </s>\n",
      "1          Other <extra_id_0> such as buckler </s>\n",
      "2        Other <extra_id_0> such as spelunker </s>\n",
      "3     Other <extra_id_0> such as quo warranto </s>\n",
      "4     Other <extra_id_0> such as Jeff Francis </s>\n",
      "Name: term, dtype: object\n",
      "0       malevolence, distaste, hatred, hate, malignity\n",
      "1                                           body armor\n",
      "2                    exploration, adventurer, explorer\n",
      "3    proceedings, legal proceedings, proceeding, du...\n",
      "4               thrower, baseball player, jock, person\n",
      "Name: hypernym, dtype: object\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1500/1500 [08:32<00:00,  2.93it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "0     My favorite <extra_id_0> is either blackfly </s>\n",
      "1     My favorite <extra_id_0> is either Turonian </s>\n",
      "2    My favorite <extra_id_0> is either abhorrence ...\n",
      "3    My favorite <extra_id_0> is either tropical st...\n",
      "4    My favorite <extra_id_0> is either militarizat...\n",
      "Name: term, dtype: object\n",
      "0                           homopterous insect, insect\n",
      "1    technical specification, geologic timescale, p...\n",
      "2                      distaste, hatred, hate, disgust\n",
      "3    atmosphere, windstorm, violent storm, air curr...\n",
      "4                                       social control\n",
      "Name: hypernym, dtype: object\n",
      "0    My favorite <extra_id_0> is either maliciousne...\n",
      "1      My favorite <extra_id_0> is either buckler </s>\n",
      "2    My favorite <extra_id_0> is either spelunker </s>\n",
      "3    My favorite <extra_id_0> is either quo warrant...\n",
      "4    My favorite <extra_id_0> is either Jeff Franci...\n",
      "Name: term, dtype: object\n",
      "0       malevolence, distaste, hatred, hate, malignity\n",
      "1                                           body armor\n",
      "2                    exploration, adventurer, explorer\n",
      "3    proceedings, legal proceedings, proceeding, du...\n",
      "4               thrower, baseball player, jock, person\n",
      "Name: hypernym, dtype: object\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1500/1500 [08:46<00:00,  2.85it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "0    There were a lot of <extra_id_0> here such as ...\n",
      "1    There were a lot of <extra_id_0> here such as ...\n",
      "2    There were a lot of <extra_id_0> here such as ...\n",
      "3    There were a lot of <extra_id_0> here such as ...\n",
      "4    There were a lot of <extra_id_0> here such as ...\n",
      "Name: term, dtype: object\n",
      "0                           homopterous insect, insect\n",
      "1    technical specification, geologic timescale, p...\n",
      "2                      distaste, hatred, hate, disgust\n",
      "3    atmosphere, windstorm, violent storm, air curr...\n",
      "4                                       social control\n",
      "Name: hypernym, dtype: object\n",
      "0    There were a lot of <extra_id_0> here such as ...\n",
      "1    There were a lot of <extra_id_0> here such as ...\n",
      "2    There were a lot of <extra_id_0> here such as ...\n",
      "3    There were a lot of <extra_id_0> here such as ...\n",
      "4    There were a lot of <extra_id_0> here such as ...\n",
      "Name: term, dtype: object\n",
      "0       malevolence, distaste, hatred, hate, malignity\n",
      "1                                           body armor\n",
      "2                    exploration, adventurer, explorer\n",
      "3    proceedings, legal proceedings, proceeding, du...\n",
      "4               thrower, baseball player, jock, person\n",
      "Name: hypernym, dtype: object\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1500/1500 [08:51<00:00,  2.82it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "0    which includes various <extra_id_0> like black...\n",
      "1    which includes various <extra_id_0> like Turon...\n",
      "2    which includes various <extra_id_0> like abhor...\n",
      "3    which includes various <extra_id_0> like tropi...\n",
      "4    which includes various <extra_id_0> like milit...\n",
      "Name: term, dtype: object\n",
      "0                           homopterous insect, insect\n",
      "1    technical specification, geologic timescale, p...\n",
      "2                      distaste, hatred, hate, disgust\n",
      "3    atmosphere, windstorm, violent storm, air curr...\n",
      "4                                       social control\n",
      "Name: hypernym, dtype: object\n",
      "0    which includes various <extra_id_0> like malic...\n",
      "1    which includes various <extra_id_0> like buckl...\n",
      "2    which includes various <extra_id_0> like spelu...\n",
      "3    which includes various <extra_id_0> like quo w...\n",
      "4    which includes various <extra_id_0> like Jeff ...\n",
      "Name: term, dtype: object\n",
      "0       malevolence, distaste, hatred, hate, malignity\n",
      "1                                           body armor\n",
      "2                    exploration, adventurer, explorer\n",
      "3    proceedings, legal proceedings, proceeding, du...\n",
      "4               thrower, baseball player, jock, person\n",
      "Name: hypernym, dtype: object\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1500/1500 [08:19<00:00,  3.00it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "0          Other <extra_id_0> especially blackfly </s>\n",
      "1          Other <extra_id_0> especially Turonian </s>\n",
      "2        Other <extra_id_0> especially abhorrence </s>\n",
      "3    Other <extra_id_0> especially tropical storm </s>\n",
      "4    Other <extra_id_0> especially militarization </s>\n",
      "Name: term, dtype: object\n",
      "0                           homopterous insect, insect\n",
      "1    technical specification, geologic timescale, p...\n",
      "2                      distaste, hatred, hate, disgust\n",
      "3    atmosphere, windstorm, violent storm, air curr...\n",
      "4                                       social control\n",
      "Name: hypernym, dtype: object\n",
      "0    Other <extra_id_0> especially maliciousness </s>\n",
      "1          Other <extra_id_0> especially buckler </s>\n",
      "2        Other <extra_id_0> especially spelunker </s>\n",
      "3     Other <extra_id_0> especially quo warranto </s>\n",
      "4     Other <extra_id_0> especially Jeff Francis </s>\n",
      "Name: term, dtype: object\n",
      "0       malevolence, distaste, hatred, hate, malignity\n",
      "1                                           body armor\n",
      "2                    exploration, adventurer, explorer\n",
      "3    proceedings, legal proceedings, proceeding, du...\n",
      "4               thrower, baseball player, jock, person\n",
      "Name: hypernym, dtype: object\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1500/1500 [08:04<00:00,  3.10it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "0    which includes various <extra_id_0> such as bl...\n",
      "1    which includes various <extra_id_0> such as Tu...\n",
      "2    which includes various <extra_id_0> such as ab...\n",
      "3    which includes various <extra_id_0> such as tr...\n",
      "4    which includes various <extra_id_0> such as mi...\n",
      "Name: term, dtype: object\n",
      "0                           homopterous insect, insect\n",
      "1    technical specification, geologic timescale, p...\n",
      "2                      distaste, hatred, hate, disgust\n",
      "3    atmosphere, windstorm, violent storm, air curr...\n",
      "4                                       social control\n",
      "Name: hypernym, dtype: object\n",
      "0    which includes various <extra_id_0> such as ma...\n",
      "1    which includes various <extra_id_0> such as bu...\n",
      "2    which includes various <extra_id_0> such as sp...\n",
      "3    which includes various <extra_id_0> such as qu...\n",
      "4    which includes various <extra_id_0> such as Je...\n",
      "Name: term, dtype: object\n",
      "0       malevolence, distaste, hatred, hate, malignity\n",
      "1                                           body armor\n",
      "2                    exploration, adventurer, explorer\n",
      "3    proceedings, legal proceedings, proceeding, du...\n",
      "4               thrower, baseball player, jock, person\n",
      "Name: hypernym, dtype: object\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1500/1500 [08:21<00:00,  2.99it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "0          My favorite <extra_id_0> is blackfly </s>\n",
      "1          My favorite <extra_id_0> is Turonian </s>\n",
      "2        My favorite <extra_id_0> is abhorrence </s>\n",
      "3    My favorite <extra_id_0> is tropical storm </s>\n",
      "4    My favorite <extra_id_0> is militarization </s>\n",
      "Name: term, dtype: object\n",
      "0                           homopterous insect, insect\n",
      "1    technical specification, geologic timescale, p...\n",
      "2                      distaste, hatred, hate, disgust\n",
      "3    atmosphere, windstorm, violent storm, air curr...\n",
      "4                                       social control\n",
      "Name: hypernym, dtype: object\n",
      "0    My favorite <extra_id_0> is maliciousness </s>\n",
      "1          My favorite <extra_id_0> is buckler </s>\n",
      "2        My favorite <extra_id_0> is spelunker </s>\n",
      "3     My favorite <extra_id_0> is quo warranto </s>\n",
      "4     My favorite <extra_id_0> is Jeff Francis </s>\n",
      "Name: term, dtype: object\n",
      "0       malevolence, distaste, hatred, hate, malignity\n",
      "1                                           body armor\n",
      "2                    exploration, adventurer, explorer\n",
      "3    proceedings, legal proceedings, proceeding, du...\n",
      "4               thrower, baseball player, jock, person\n",
      "Name: hypernym, dtype: object\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1500/1500 [07:55<00:00,  3.15it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "0    I know such types of <extra_id_0> as blackfly ...\n",
      "1    I know such types of <extra_id_0> as Turonian ...\n",
      "2    I know such types of <extra_id_0> as abhorrenc...\n",
      "3    I know such types of <extra_id_0> as tropical ...\n",
      "4    I know such types of <extra_id_0> as militariz...\n",
      "Name: term, dtype: object\n",
      "0                           homopterous insect, insect\n",
      "1    technical specification, geologic timescale, p...\n",
      "2                      distaste, hatred, hate, disgust\n",
      "3    atmosphere, windstorm, violent storm, air curr...\n",
      "4                                       social control\n",
      "Name: hypernym, dtype: object\n",
      "0    I know such types of <extra_id_0> as malicious...\n",
      "1    I know such types of <extra_id_0> as buckler </s>\n",
      "2    I know such types of <extra_id_0> as spelunker...\n",
      "3    I know such types of <extra_id_0> as quo warra...\n",
      "4    I know such types of <extra_id_0> as Jeff Fran...\n",
      "Name: term, dtype: object\n",
      "0       malevolence, distaste, hatred, hate, malignity\n",
      "1                                           body armor\n",
      "2                    exploration, adventurer, explorer\n",
      "3    proceedings, legal proceedings, proceeding, du...\n",
      "4               thrower, baseball player, jock, person\n",
      "Name: hypernym, dtype: object\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1500/1500 [08:59<00:00,  2.78it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "0    I know such kinds of <extra_id_0> as blackfly ...\n",
      "1    I know such kinds of <extra_id_0> as Turonian ...\n",
      "2    I know such kinds of <extra_id_0> as abhorrenc...\n",
      "3    I know such kinds of <extra_id_0> as tropical ...\n",
      "4    I know such kinds of <extra_id_0> as militariz...\n",
      "Name: term, dtype: object\n",
      "0                           homopterous insect, insect\n",
      "1    technical specification, geologic timescale, p...\n",
      "2                      distaste, hatred, hate, disgust\n",
      "3    atmosphere, windstorm, violent storm, air curr...\n",
      "4                                       social control\n",
      "Name: hypernym, dtype: object\n",
      "0    I know such kinds of <extra_id_0> as malicious...\n",
      "1    I know such kinds of <extra_id_0> as buckler </s>\n",
      "2    I know such kinds of <extra_id_0> as spelunker...\n",
      "3    I know such kinds of <extra_id_0> as quo warra...\n",
      "4    I know such kinds of <extra_id_0> as Jeff Fran...\n",
      "Name: term, dtype: object\n",
      "0       malevolence, distaste, hatred, hate, malignity\n",
      "1                                           body armor\n",
      "2                    exploration, adventurer, explorer\n",
      "3    proceedings, legal proceedings, proceeding, du...\n",
      "4               thrower, baseball player, jock, person\n",
      "Name: hypernym, dtype: object\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1500/1500 [08:43<00:00,  2.86it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "0          <extra_id_0> such as blackfly </s>\n",
      "1          <extra_id_0> such as Turonian </s>\n",
      "2        <extra_id_0> such as abhorrence </s>\n",
      "3    <extra_id_0> such as tropical storm </s>\n",
      "4    <extra_id_0> such as militarization </s>\n",
      "Name: term, dtype: object\n",
      "0                           homopterous insect, insect\n",
      "1    technical specification, geologic timescale, p...\n",
      "2                      distaste, hatred, hate, disgust\n",
      "3    atmosphere, windstorm, violent storm, air curr...\n",
      "4                                       social control\n",
      "Name: hypernym, dtype: object\n",
      "0    <extra_id_0> such as maliciousness </s>\n",
      "1          <extra_id_0> such as buckler </s>\n",
      "2        <extra_id_0> such as spelunker </s>\n",
      "3     <extra_id_0> such as quo warranto </s>\n",
      "4     <extra_id_0> such as Jeff Francis </s>\n",
      "Name: term, dtype: object\n",
      "0       malevolence, distaste, hatred, hate, malignity\n",
      "1                                           body armor\n",
      "2                    exploration, adventurer, explorer\n",
      "3    proceedings, legal proceedings, proceeding, du...\n",
      "4               thrower, baseball player, jock, person\n",
      "Name: hypernym, dtype: object\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1500/1500 [08:24<00:00,  2.97it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "0    I know many kinds of <extra_id_0> for example ...\n",
      "1    I know many kinds of <extra_id_0> for example ...\n",
      "2    I know many kinds of <extra_id_0> for example ...\n",
      "3    I know many kinds of <extra_id_0> for example ...\n",
      "4    I know many kinds of <extra_id_0> for example ...\n",
      "Name: term, dtype: object\n",
      "0                           homopterous insect, insect\n",
      "1    technical specification, geologic timescale, p...\n",
      "2                      distaste, hatred, hate, disgust\n",
      "3    atmosphere, windstorm, violent storm, air curr...\n",
      "4                                       social control\n",
      "Name: hypernym, dtype: object\n",
      "0    I know many kinds of <extra_id_0> for example ...\n",
      "1    I know many kinds of <extra_id_0> for example ...\n",
      "2    I know many kinds of <extra_id_0> for example ...\n",
      "3    I know many kinds of <extra_id_0> for example ...\n",
      "4    I know many kinds of <extra_id_0> for example ...\n",
      "Name: term, dtype: object\n",
      "0       malevolence, distaste, hatred, hate, malignity\n",
      "1                                           body armor\n",
      "2                    exploration, adventurer, explorer\n",
      "3    proceedings, legal proceedings, proceeding, du...\n",
      "4               thrower, baseball player, jock, person\n",
      "Name: hypernym, dtype: object\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1500/1500 [08:55<00:00,  2.80it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "0         Other <extra_id_0> for example blackfly </s>\n",
      "1         Other <extra_id_0> for example Turonian </s>\n",
      "2       Other <extra_id_0> for example abhorrence </s>\n",
      "3    Other <extra_id_0> for example tropical storm ...\n",
      "4    Other <extra_id_0> for example militarization ...\n",
      "Name: term, dtype: object\n",
      "0                           homopterous insect, insect\n",
      "1    technical specification, geologic timescale, p...\n",
      "2                      distaste, hatred, hate, disgust\n",
      "3    atmosphere, windstorm, violent storm, air curr...\n",
      "4                                       social control\n",
      "Name: hypernym, dtype: object\n",
      "0    Other <extra_id_0> for example maliciousness </s>\n",
      "1          Other <extra_id_0> for example buckler </s>\n",
      "2        Other <extra_id_0> for example spelunker </s>\n",
      "3     Other <extra_id_0> for example quo warranto </s>\n",
      "4     Other <extra_id_0> for example Jeff Francis </s>\n",
      "Name: term, dtype: object\n",
      "0       malevolence, distaste, hatred, hate, malignity\n",
      "1                                           body armor\n",
      "2                    exploration, adventurer, explorer\n",
      "3    proceedings, legal proceedings, proceeding, du...\n",
      "4               thrower, baseball player, jock, person\n",
      "Name: hypernym, dtype: object\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1500/1500 [08:20<00:00,  3.00it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "0          <extra_id_0> namely blackfly </s>\n",
      "1          <extra_id_0> namely Turonian </s>\n",
      "2        <extra_id_0> namely abhorrence </s>\n",
      "3    <extra_id_0> namely tropical storm </s>\n",
      "4    <extra_id_0> namely militarization </s>\n",
      "Name: term, dtype: object\n",
      "0                           homopterous insect, insect\n",
      "1    technical specification, geologic timescale, p...\n",
      "2                      distaste, hatred, hate, disgust\n",
      "3    atmosphere, windstorm, violent storm, air curr...\n",
      "4                                       social control\n",
      "Name: hypernym, dtype: object\n",
      "0    <extra_id_0> namely maliciousness </s>\n",
      "1          <extra_id_0> namely buckler </s>\n",
      "2        <extra_id_0> namely spelunker </s>\n",
      "3     <extra_id_0> namely quo warranto </s>\n",
      "4     <extra_id_0> namely Jeff Francis </s>\n",
      "Name: term, dtype: object\n",
      "0       malevolence, distaste, hatred, hate, malignity\n",
      "1                                           body armor\n",
      "2                    exploration, adventurer, explorer\n",
      "3    proceedings, legal proceedings, proceeding, du...\n",
      "4               thrower, baseball player, jock, person\n",
      "Name: hypernym, dtype: object\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1500/1500 [08:21<00:00,  2.99it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "0    I know many types of <extra_id_0> for example ...\n",
      "1    I know many types of <extra_id_0> for example ...\n",
      "2    I know many types of <extra_id_0> for example ...\n",
      "3    I know many types of <extra_id_0> for example ...\n",
      "4    I know many types of <extra_id_0> for example ...\n",
      "Name: term, dtype: object\n",
      "0                           homopterous insect, insect\n",
      "1    technical specification, geologic timescale, p...\n",
      "2                      distaste, hatred, hate, disgust\n",
      "3    atmosphere, windstorm, violent storm, air curr...\n",
      "4                                       social control\n",
      "Name: hypernym, dtype: object\n",
      "0    I know many types of <extra_id_0> for example ...\n",
      "1    I know many types of <extra_id_0> for example ...\n",
      "2    I know many types of <extra_id_0> for example ...\n",
      "3    I know many types of <extra_id_0> for example ...\n",
      "4    I know many types of <extra_id_0> for example ...\n",
      "Name: term, dtype: object\n",
      "0       malevolence, distaste, hatred, hate, malignity\n",
      "1                                           body armor\n",
      "2                    exploration, adventurer, explorer\n",
      "3    proceedings, legal proceedings, proceeding, du...\n",
      "4               thrower, baseball player, jock, person\n",
      "Name: hypernym, dtype: object\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1500/1500 [08:27<00:00,  2.96it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "0          <extra_id_0> including blackfly </s>\n",
      "1          <extra_id_0> including Turonian </s>\n",
      "2        <extra_id_0> including abhorrence </s>\n",
      "3    <extra_id_0> including tropical storm </s>\n",
      "4    <extra_id_0> including militarization </s>\n",
      "Name: term, dtype: object\n",
      "0                           homopterous insect, insect\n",
      "1    technical specification, geologic timescale, p...\n",
      "2                      distaste, hatred, hate, disgust\n",
      "3    atmosphere, windstorm, violent storm, air curr...\n",
      "4                                       social control\n",
      "Name: hypernym, dtype: object\n",
      "0    <extra_id_0> including maliciousness </s>\n",
      "1          <extra_id_0> including buckler </s>\n",
      "2        <extra_id_0> including spelunker </s>\n",
      "3     <extra_id_0> including quo warranto </s>\n",
      "4     <extra_id_0> including Jeff Francis </s>\n",
      "Name: term, dtype: object\n",
      "0       malevolence, distaste, hatred, hate, malignity\n",
      "1                                           body armor\n",
      "2                    exploration, adventurer, explorer\n",
      "3    proceedings, legal proceedings, proceeding, du...\n",
      "4               thrower, baseball player, jock, person\n",
      "Name: hypernym, dtype: object\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1500/1500 [08:40<00:00,  2.88it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "0    There are a lot of <extra_id_0> for example bl...\n",
      "1    There are a lot of <extra_id_0> for example Tu...\n",
      "2    There are a lot of <extra_id_0> for example ab...\n",
      "3    There are a lot of <extra_id_0> for example tr...\n",
      "4    There are a lot of <extra_id_0> for example mi...\n",
      "Name: term, dtype: object\n",
      "0                           homopterous insect, insect\n",
      "1    technical specification, geologic timescale, p...\n",
      "2                      distaste, hatred, hate, disgust\n",
      "3    atmosphere, windstorm, violent storm, air curr...\n",
      "4                                       social control\n",
      "Name: hypernym, dtype: object\n",
      "0    There are a lot of <extra_id_0> for example ma...\n",
      "1    There are a lot of <extra_id_0> for example bu...\n",
      "2    There are a lot of <extra_id_0> for example sp...\n",
      "3    There are a lot of <extra_id_0> for example qu...\n",
      "4    There are a lot of <extra_id_0> for example Je...\n",
      "Name: term, dtype: object\n",
      "0       malevolence, distaste, hatred, hate, malignity\n",
      "1                                           body armor\n",
      "2                    exploration, adventurer, explorer\n",
      "3    proceedings, legal proceedings, proceeding, du...\n",
      "4               thrower, baseball player, jock, person\n",
      "Name: hypernym, dtype: object\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1500/1500 [08:48<00:00,  2.84it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "0    which includes various <extra_id_0> for exampl...\n",
      "1    which includes various <extra_id_0> for exampl...\n",
      "2    which includes various <extra_id_0> for exampl...\n",
      "3    which includes various <extra_id_0> for exampl...\n",
      "4    which includes various <extra_id_0> for exampl...\n",
      "Name: term, dtype: object\n",
      "0                           homopterous insect, insect\n",
      "1    technical specification, geologic timescale, p...\n",
      "2                      distaste, hatred, hate, disgust\n",
      "3    atmosphere, windstorm, violent storm, air curr...\n",
      "4                                       social control\n",
      "Name: hypernym, dtype: object\n",
      "0    which includes various <extra_id_0> for exampl...\n",
      "1    which includes various <extra_id_0> for exampl...\n",
      "2    which includes various <extra_id_0> for exampl...\n",
      "3    which includes various <extra_id_0> for exampl...\n",
      "4    which includes various <extra_id_0> for exampl...\n",
      "Name: term, dtype: object\n",
      "0       malevolence, distaste, hatred, hate, malignity\n",
      "1                                           body armor\n",
      "2                    exploration, adventurer, explorer\n",
      "3    proceedings, legal proceedings, proceeding, du...\n",
      "4               thrower, baseball player, jock, person\n",
      "Name: hypernym, dtype: object\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1500/1500 [08:29<00:00,  2.94it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "0    There are a lot of <extra_id_0> here for examp...\n",
      "1    There are a lot of <extra_id_0> here for examp...\n",
      "2    There are a lot of <extra_id_0> here for examp...\n",
      "3    There are a lot of <extra_id_0> here for examp...\n",
      "4    There are a lot of <extra_id_0> here for examp...\n",
      "Name: term, dtype: object\n",
      "0                           homopterous insect, insect\n",
      "1    technical specification, geologic timescale, p...\n",
      "2                      distaste, hatred, hate, disgust\n",
      "3    atmosphere, windstorm, violent storm, air curr...\n",
      "4                                       social control\n",
      "Name: hypernym, dtype: object\n",
      "0    There are a lot of <extra_id_0> here for examp...\n",
      "1    There are a lot of <extra_id_0> here for examp...\n",
      "2    There are a lot of <extra_id_0> here for examp...\n",
      "3    There are a lot of <extra_id_0> here for examp...\n",
      "4    There are a lot of <extra_id_0> here for examp...\n",
      "Name: term, dtype: object\n",
      "0       malevolence, distaste, hatred, hate, malignity\n",
      "1                                           body armor\n",
      "2                    exploration, adventurer, explorer\n",
      "3    proceedings, legal proceedings, proceeding, du...\n",
      "4               thrower, baseball player, jock, person\n",
      "Name: hypernym, dtype: object\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1500/1500 [09:01<00:00,  2.77it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "0          <extra_id_0> e.g. blackfly </s>\n",
      "1          <extra_id_0> e.g. Turonian </s>\n",
      "2        <extra_id_0> e.g. abhorrence </s>\n",
      "3    <extra_id_0> e.g. tropical storm </s>\n",
      "4    <extra_id_0> e.g. militarization </s>\n",
      "Name: term, dtype: object\n",
      "0                           homopterous insect, insect\n",
      "1    technical specification, geologic timescale, p...\n",
      "2                      distaste, hatred, hate, disgust\n",
      "3    atmosphere, windstorm, violent storm, air curr...\n",
      "4                                       social control\n",
      "Name: hypernym, dtype: object\n",
      "0    <extra_id_0> e.g. maliciousness </s>\n",
      "1          <extra_id_0> e.g. buckler </s>\n",
      "2        <extra_id_0> e.g. spelunker </s>\n",
      "3     <extra_id_0> e.g. quo warranto </s>\n",
      "4     <extra_id_0> e.g. Jeff Francis </s>\n",
      "Name: term, dtype: object\n",
      "0       malevolence, distaste, hatred, hate, malignity\n",
      "1                                           body armor\n",
      "2                    exploration, adventurer, explorer\n",
      "3    proceedings, legal proceedings, proceeding, du...\n",
      "4               thrower, baseball player, jock, person\n",
      "Name: hypernym, dtype: object\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1500/1500 [08:51<00:00,  2.82it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "0          <extra_id_0> like blackfly </s>\n",
      "1          <extra_id_0> like Turonian </s>\n",
      "2        <extra_id_0> like abhorrence </s>\n",
      "3    <extra_id_0> like tropical storm </s>\n",
      "4    <extra_id_0> like militarization </s>\n",
      "Name: term, dtype: object\n",
      "0                           homopterous insect, insect\n",
      "1    technical specification, geologic timescale, p...\n",
      "2                      distaste, hatred, hate, disgust\n",
      "3    atmosphere, windstorm, violent storm, air curr...\n",
      "4                                       social control\n",
      "Name: hypernym, dtype: object\n",
      "0    <extra_id_0> like maliciousness </s>\n",
      "1          <extra_id_0> like buckler </s>\n",
      "2        <extra_id_0> like spelunker </s>\n",
      "3     <extra_id_0> like quo warranto </s>\n",
      "4     <extra_id_0> like Jeff Francis </s>\n",
      "Name: term, dtype: object\n",
      "0       malevolence, distaste, hatred, hate, malignity\n",
      "1                                           body armor\n",
      "2                    exploration, adventurer, explorer\n",
      "3    proceedings, legal proceedings, proceeding, du...\n",
      "4               thrower, baseball player, jock, person\n",
      "Name: hypernym, dtype: object\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1500/1500 [08:35<00:00,  2.91it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "0          <extra_id_0> especially blackfly </s>\n",
      "1          <extra_id_0> especially Turonian </s>\n",
      "2        <extra_id_0> especially abhorrence </s>\n",
      "3    <extra_id_0> especially tropical storm </s>\n",
      "4    <extra_id_0> especially militarization </s>\n",
      "Name: term, dtype: object\n",
      "0                           homopterous insect, insect\n",
      "1    technical specification, geologic timescale, p...\n",
      "2                      distaste, hatred, hate, disgust\n",
      "3    atmosphere, windstorm, violent storm, air curr...\n",
      "4                                       social control\n",
      "Name: hypernym, dtype: object\n",
      "0    <extra_id_0> especially maliciousness </s>\n",
      "1          <extra_id_0> especially buckler </s>\n",
      "2        <extra_id_0> especially spelunker </s>\n",
      "3     <extra_id_0> especially quo warranto </s>\n",
      "4     <extra_id_0> especially Jeff Francis </s>\n",
      "Name: term, dtype: object\n",
      "0       malevolence, distaste, hatred, hate, malignity\n",
      "1                                           body armor\n",
      "2                    exploration, adventurer, explorer\n",
      "3    proceedings, legal proceedings, proceeding, du...\n",
      "4               thrower, baseball player, jock, person\n",
      "Name: hypernym, dtype: object\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1500/1500 [09:36<00:00,  2.60it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "0          <extra_id_0> for example blackfly </s>\n",
      "1          <extra_id_0> for example Turonian </s>\n",
      "2        <extra_id_0> for example abhorrence </s>\n",
      "3    <extra_id_0> for example tropical storm </s>\n",
      "4    <extra_id_0> for example militarization </s>\n",
      "Name: term, dtype: object\n",
      "0                           homopterous insect, insect\n",
      "1    technical specification, geologic timescale, p...\n",
      "2                      distaste, hatred, hate, disgust\n",
      "3    atmosphere, windstorm, violent storm, air curr...\n",
      "4                                       social control\n",
      "Name: hypernym, dtype: object\n",
      "0    <extra_id_0> for example maliciousness </s>\n",
      "1          <extra_id_0> for example buckler </s>\n",
      "2        <extra_id_0> for example spelunker </s>\n",
      "3     <extra_id_0> for example quo warranto </s>\n",
      "4     <extra_id_0> for example Jeff Francis </s>\n",
      "Name: term, dtype: object\n",
      "0       malevolence, distaste, hatred, hate, malignity\n",
      "1                                           body armor\n",
      "2                    exploration, adventurer, explorer\n",
      "3    proceedings, legal proceedings, proceeding, du...\n",
      "4               thrower, baseball player, jock, person\n",
      "Name: hypernym, dtype: object\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1500/1500 [08:59<00:00,  2.78it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "0          <extra_id_0> for instance blackfly </s>\n",
      "1          <extra_id_0> for instance Turonian </s>\n",
      "2        <extra_id_0> for instance abhorrence </s>\n",
      "3    <extra_id_0> for instance tropical storm </s>\n",
      "4    <extra_id_0> for instance militarization </s>\n",
      "Name: term, dtype: object\n",
      "0                           homopterous insect, insect\n",
      "1    technical specification, geologic timescale, p...\n",
      "2                      distaste, hatred, hate, disgust\n",
      "3    atmosphere, windstorm, violent storm, air curr...\n",
      "4                                       social control\n",
      "Name: hypernym, dtype: object\n",
      "0    <extra_id_0> for instance maliciousness </s>\n",
      "1          <extra_id_0> for instance buckler </s>\n",
      "2        <extra_id_0> for instance spelunker </s>\n",
      "3     <extra_id_0> for instance quo warranto </s>\n",
      "4     <extra_id_0> for instance Jeff Francis </s>\n",
      "Name: term, dtype: object\n",
      "0       malevolence, distaste, hatred, hate, malignity\n",
      "1                                           body armor\n",
      "2                    exploration, adventurer, explorer\n",
      "3    proceedings, legal proceedings, proceeding, du...\n",
      "4               thrower, baseball player, jock, person\n",
      "Name: hypernym, dtype: object\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1500/1500 [08:34<00:00,  2.92it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    }
   ],
   "source": [
    "hearst_patterns_table = []\n",
    "\n",
    "for pattern in hearst_patterns_in_use:\n",
    "    \n",
    "#   Data processing\n",
    "    train_data_en, train_gold_en, test_data_en, test_gold_en = hearest_preprocessing(train_data_en_data,\n",
    "                                                                                     train_gold_en_data, \n",
    "                                                                                     test_data_en_data, \n",
    "                                                                                     test_gold_en_data, \n",
    "                                                                                     pattern)\n",
    "#   Predict\n",
    "    predict(test_data_en, test_gold_en)\n",
    "    \n",
    "#   Metrics\n",
    "    \n",
    "    f = io.StringIO()\n",
    "    with redirect_stdout(f):\n",
    "        !python debuged_task9-scorer.py SemEval2018-Task9/test/gold/1A.english.test.gold.txt pred_hearst_pattern.txt\n",
    "\n",
    "\n",
    "    _std_out = f.getvalue()\n",
    "\n",
    "\n",
    "    _std_out = _std_out  + pattern\n",
    "    \n",
    "    pattern_table = answers(_std_out, pattern)\n",
    "    hearst_patterns_table.append(pattern_table)\n",
    "\n",
    "\n",
    "    df = pd.concat(hearst_patterns_table)\n",
    "    df.to_csv('hearst_patterns_result_table.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "f10cedf6-bc4b-498c-bbcf-9d2abf64e690",
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics_table = pd.read_csv('hearst_patterns_result_table.csv').set_index('name of hearst pattern')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "8a0d1546-10af-4a1a-92d5-ac0a8da02147",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style type=\"text/css\">\n",
       "#T_0576a_row0_col5, #T_0576a_row3_col2, #T_0576a_row6_col2, #T_0576a_row18_col0, #T_0576a_row18_col1, #T_0576a_row18_col3, #T_0576a_row18_col4 {\n",
       "  color: white;\n",
       "  background-color: #FF5555;\n",
       "}\n",
       "#T_0576a_row2_col2, #T_0576a_row7_col0, #T_0576a_row7_col1, #T_0576a_row7_col2, #T_0576a_row7_col3, #T_0576a_row7_col4, #T_0576a_row7_col5 {\n",
       "  color: white;\n",
       "  background-color: #1FC29D;\n",
       "}\n",
       "</style>\n",
       "<table id=\"T_0576a\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th class=\"blank level0\" >&nbsp;</th>\n",
       "      <th id=\"T_0576a_level0_col0\" class=\"col_heading level0 col0\" >MRR</th>\n",
       "      <th id=\"T_0576a_level0_col1\" class=\"col_heading level0 col1\" >MAP</th>\n",
       "      <th id=\"T_0576a_level0_col2\" class=\"col_heading level0 col2\" >P@1</th>\n",
       "      <th id=\"T_0576a_level0_col3\" class=\"col_heading level0 col3\" >P@3</th>\n",
       "      <th id=\"T_0576a_level0_col4\" class=\"col_heading level0 col4\" >P@5</th>\n",
       "      <th id=\"T_0576a_level0_col5\" class=\"col_heading level0 col5\" >P@15</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th class=\"index_name level0\" >name of hearst pattern</th>\n",
       "      <th class=\"blank col0\" >&nbsp;</th>\n",
       "      <th class=\"blank col1\" >&nbsp;</th>\n",
       "      <th class=\"blank col2\" >&nbsp;</th>\n",
       "      <th class=\"blank col3\" >&nbsp;</th>\n",
       "      <th class=\"blank col4\" >&nbsp;</th>\n",
       "      <th class=\"blank col5\" >&nbsp;</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th id=\"T_0576a_level0_row0\" class=\"row_heading level0 row0\" >There are a lot of [PARENT] here such as</th>\n",
       "      <td id=\"T_0576a_row0_col0\" class=\"data row0 col0\" >0.001690</td>\n",
       "      <td id=\"T_0576a_row0_col1\" class=\"data row0 col1\" >0.000710</td>\n",
       "      <td id=\"T_0576a_row0_col2\" class=\"data row0 col2\" >0.000670</td>\n",
       "      <td id=\"T_0576a_row0_col3\" class=\"data row0 col3\" >0.000890</td>\n",
       "      <td id=\"T_0576a_row0_col4\" class=\"data row0 col4\" >0.000600</td>\n",
       "      <td id=\"T_0576a_row0_col5\" class=\"data row0 col5\" >0.000910</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_0576a_level0_row1\" class=\"row_heading level0 row1\" >Other [PARENT] such as</th>\n",
       "      <td id=\"T_0576a_row1_col0\" class=\"data row1 col0\" >0.020890</td>\n",
       "      <td id=\"T_0576a_row1_col1\" class=\"data row1 col1\" >0.013760</td>\n",
       "      <td id=\"T_0576a_row1_col2\" class=\"data row1 col2\" >0.008670</td>\n",
       "      <td id=\"T_0576a_row1_col3\" class=\"data row1 col3\" >0.011560</td>\n",
       "      <td id=\"T_0576a_row1_col4\" class=\"data row1 col4\" >0.013100</td>\n",
       "      <td id=\"T_0576a_row1_col5\" class=\"data row1 col5\" >0.015720</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_0576a_level0_row2\" class=\"row_heading level0 row2\" >My favorite [PARENT] is either</th>\n",
       "      <td id=\"T_0576a_row2_col0\" class=\"data row2 col0\" >0.168910</td>\n",
       "      <td id=\"T_0576a_row2_col1\" class=\"data row2 col1\" >0.093550</td>\n",
       "      <td id=\"T_0576a_row2_col2\" class=\"data row2 col2\" >0.114670</td>\n",
       "      <td id=\"T_0576a_row2_col3\" class=\"data row2 col3\" >0.090110</td>\n",
       "      <td id=\"T_0576a_row2_col4\" class=\"data row2 col4\" >0.086810</td>\n",
       "      <td id=\"T_0576a_row2_col5\" class=\"data row2 col5\" >0.095720</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_0576a_level0_row3\" class=\"row_heading level0 row3\" >There were a lot of [PARENT] here such as</th>\n",
       "      <td id=\"T_0576a_row3_col0\" class=\"data row3 col0\" >0.001780</td>\n",
       "      <td id=\"T_0576a_row3_col1\" class=\"data row3 col1\" >0.001190</td>\n",
       "      <td id=\"T_0576a_row3_col2\" class=\"data row3 col2\" >0.000000</td>\n",
       "      <td id=\"T_0576a_row3_col3\" class=\"data row3 col3\" >0.001000</td>\n",
       "      <td id=\"T_0576a_row3_col4\" class=\"data row3 col4\" >0.001070</td>\n",
       "      <td id=\"T_0576a_row3_col5\" class=\"data row3 col5\" >0.001600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_0576a_level0_row4\" class=\"row_heading level0 row4\" >which includes various [PARENT] like</th>\n",
       "      <td id=\"T_0576a_row4_col0\" class=\"data row4 col0\" >0.007870</td>\n",
       "      <td id=\"T_0576a_row4_col1\" class=\"data row4 col1\" >0.006150</td>\n",
       "      <td id=\"T_0576a_row4_col2\" class=\"data row4 col2\" >0.001330</td>\n",
       "      <td id=\"T_0576a_row4_col3\" class=\"data row4 col3\" >0.004110</td>\n",
       "      <td id=\"T_0576a_row4_col4\" class=\"data row4 col4\" >0.004870</td>\n",
       "      <td id=\"T_0576a_row4_col5\" class=\"data row4 col5\" >0.009190</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_0576a_level0_row5\" class=\"row_heading level0 row5\" >Other [PARENT] especially</th>\n",
       "      <td id=\"T_0576a_row5_col0\" class=\"data row5 col0\" >0.062890</td>\n",
       "      <td id=\"T_0576a_row5_col1\" class=\"data row5 col1\" >0.035380</td>\n",
       "      <td id=\"T_0576a_row5_col2\" class=\"data row5 col2\" >0.040000</td>\n",
       "      <td id=\"T_0576a_row5_col3\" class=\"data row5 col3\" >0.029670</td>\n",
       "      <td id=\"T_0576a_row5_col4\" class=\"data row5 col4\" >0.033300</td>\n",
       "      <td id=\"T_0576a_row5_col5\" class=\"data row5 col5\" >0.037260</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_0576a_level0_row6\" class=\"row_heading level0 row6\" >which includes various [PARENT] such as</th>\n",
       "      <td id=\"T_0576a_row6_col0\" class=\"data row6 col0\" >0.004560</td>\n",
       "      <td id=\"T_0576a_row6_col1\" class=\"data row6 col1\" >0.003770</td>\n",
       "      <td id=\"T_0576a_row6_col2\" class=\"data row6 col2\" >0.000000</td>\n",
       "      <td id=\"T_0576a_row6_col3\" class=\"data row6 col3\" >0.002890</td>\n",
       "      <td id=\"T_0576a_row6_col4\" class=\"data row6 col4\" >0.003270</td>\n",
       "      <td id=\"T_0576a_row6_col5\" class=\"data row6 col5\" >0.005400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_0576a_level0_row7\" class=\"row_heading level0 row7\" >My favorite [PARENT] is</th>\n",
       "      <td id=\"T_0576a_row7_col0\" class=\"data row7 col0\" >0.180120</td>\n",
       "      <td id=\"T_0576a_row7_col1\" class=\"data row7 col1\" >0.108980</td>\n",
       "      <td id=\"T_0576a_row7_col2\" class=\"data row7 col2\" >0.114670</td>\n",
       "      <td id=\"T_0576a_row7_col3\" class=\"data row7 col3\" >0.095330</td>\n",
       "      <td id=\"T_0576a_row7_col4\" class=\"data row7 col4\" >0.094000</td>\n",
       "      <td id=\"T_0576a_row7_col5\" class=\"data row7 col5\" >0.121550</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_0576a_level0_row8\" class=\"row_heading level0 row8\" >I know such types of [PARENT] as</th>\n",
       "      <td id=\"T_0576a_row8_col0\" class=\"data row8 col0\" >0.089240</td>\n",
       "      <td id=\"T_0576a_row8_col1\" class=\"data row8 col1\" >0.059250</td>\n",
       "      <td id=\"T_0576a_row8_col2\" class=\"data row8 col2\" >0.040000</td>\n",
       "      <td id=\"T_0576a_row8_col3\" class=\"data row8 col3\" >0.049330</td>\n",
       "      <td id=\"T_0576a_row8_col4\" class=\"data row8 col4\" >0.053100</td>\n",
       "      <td id=\"T_0576a_row8_col5\" class=\"data row8 col5\" >0.068230</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_0576a_level0_row9\" class=\"row_heading level0 row9\" >I know such kinds of [PARENT] as</th>\n",
       "      <td id=\"T_0576a_row9_col0\" class=\"data row9 col0\" >0.072820</td>\n",
       "      <td id=\"T_0576a_row9_col1\" class=\"data row9 col1\" >0.054650</td>\n",
       "      <td id=\"T_0576a_row9_col2\" class=\"data row9 col2\" >0.024670</td>\n",
       "      <td id=\"T_0576a_row9_col3\" class=\"data row9 col3\" >0.039890</td>\n",
       "      <td id=\"T_0576a_row9_col4\" class=\"data row9 col4\" >0.048660</td>\n",
       "      <td id=\"T_0576a_row9_col5\" class=\"data row9 col5\" >0.067510</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_0576a_level0_row10\" class=\"row_heading level0 row10\" >[PARENT] such as</th>\n",
       "      <td id=\"T_0576a_row10_col0\" class=\"data row10 col0\" >0.027090</td>\n",
       "      <td id=\"T_0576a_row10_col1\" class=\"data row10 col1\" >0.014620</td>\n",
       "      <td id=\"T_0576a_row10_col2\" class=\"data row10 col2\" >0.014670</td>\n",
       "      <td id=\"T_0576a_row10_col3\" class=\"data row10 col3\" >0.013220</td>\n",
       "      <td id=\"T_0576a_row10_col4\" class=\"data row10 col4\" >0.014190</td>\n",
       "      <td id=\"T_0576a_row10_col5\" class=\"data row10 col5\" >0.015410</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_0576a_level0_row11\" class=\"row_heading level0 row11\" >I know many kinds of [PARENT] for example</th>\n",
       "      <td id=\"T_0576a_row11_col0\" class=\"data row11 col0\" >0.106140</td>\n",
       "      <td id=\"T_0576a_row11_col1\" class=\"data row11 col1\" >0.063550</td>\n",
       "      <td id=\"T_0576a_row11_col2\" class=\"data row11 col2\" >0.056670</td>\n",
       "      <td id=\"T_0576a_row11_col3\" class=\"data row11 col3\" >0.059000</td>\n",
       "      <td id=\"T_0576a_row11_col4\" class=\"data row11 col4\" >0.061570</td>\n",
       "      <td id=\"T_0576a_row11_col5\" class=\"data row11 col5\" >0.066090</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_0576a_level0_row12\" class=\"row_heading level0 row12\" >Other [PARENT] for example</th>\n",
       "      <td id=\"T_0576a_row12_col0\" class=\"data row12 col0\" >0.034900</td>\n",
       "      <td id=\"T_0576a_row12_col1\" class=\"data row12 col1\" >0.023340</td>\n",
       "      <td id=\"T_0576a_row12_col2\" class=\"data row12 col2\" >0.012000</td>\n",
       "      <td id=\"T_0576a_row12_col3\" class=\"data row12 col3\" >0.018780</td>\n",
       "      <td id=\"T_0576a_row12_col4\" class=\"data row12 col4\" >0.021260</td>\n",
       "      <td id=\"T_0576a_row12_col5\" class=\"data row12 col5\" >0.027280</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_0576a_level0_row13\" class=\"row_heading level0 row13\" >[PARENT] namely</th>\n",
       "      <td id=\"T_0576a_row13_col0\" class=\"data row13 col0\" >0.078530</td>\n",
       "      <td id=\"T_0576a_row13_col1\" class=\"data row13 col1\" >0.045000</td>\n",
       "      <td id=\"T_0576a_row13_col2\" class=\"data row13 col2\" >0.052670</td>\n",
       "      <td id=\"T_0576a_row13_col3\" class=\"data row13 col3\" >0.039220</td>\n",
       "      <td id=\"T_0576a_row13_col4\" class=\"data row13 col4\" >0.043010</td>\n",
       "      <td id=\"T_0576a_row13_col5\" class=\"data row13 col5\" >0.046590</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_0576a_level0_row14\" class=\"row_heading level0 row14\" >I know many types of [PARENT] for example</th>\n",
       "      <td id=\"T_0576a_row14_col0\" class=\"data row14 col0\" >0.101450</td>\n",
       "      <td id=\"T_0576a_row14_col1\" class=\"data row14 col1\" >0.059380</td>\n",
       "      <td id=\"T_0576a_row14_col2\" class=\"data row14 col2\" >0.054670</td>\n",
       "      <td id=\"T_0576a_row14_col3\" class=\"data row14 col3\" >0.055440</td>\n",
       "      <td id=\"T_0576a_row14_col4\" class=\"data row14 col4\" >0.057720</td>\n",
       "      <td id=\"T_0576a_row14_col5\" class=\"data row14 col5\" >0.061200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_0576a_level0_row15\" class=\"row_heading level0 row15\" >[PARENT] including</th>\n",
       "      <td id=\"T_0576a_row15_col0\" class=\"data row15 col0\" >0.042810</td>\n",
       "      <td id=\"T_0576a_row15_col1\" class=\"data row15 col1\" >0.022220</td>\n",
       "      <td id=\"T_0576a_row15_col2\" class=\"data row15 col2\" >0.028000</td>\n",
       "      <td id=\"T_0576a_row15_col3\" class=\"data row15 col3\" >0.020220</td>\n",
       "      <td id=\"T_0576a_row15_col4\" class=\"data row15 col4\" >0.021110</td>\n",
       "      <td id=\"T_0576a_row15_col5\" class=\"data row15 col5\" >0.022710</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_0576a_level0_row16\" class=\"row_heading level0 row16\" >There are a lot of [PARENT] for example</th>\n",
       "      <td id=\"T_0576a_row16_col0\" class=\"data row16 col0\" >0.003750</td>\n",
       "      <td id=\"T_0576a_row16_col1\" class=\"data row16 col1\" >0.001730</td>\n",
       "      <td id=\"T_0576a_row16_col2\" class=\"data row16 col2\" >0.002000</td>\n",
       "      <td id=\"T_0576a_row16_col3\" class=\"data row16 col3\" >0.001330</td>\n",
       "      <td id=\"T_0576a_row16_col4\" class=\"data row16 col4\" >0.001270</td>\n",
       "      <td id=\"T_0576a_row16_col5\" class=\"data row16 col5\" >0.002000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_0576a_level0_row17\" class=\"row_heading level0 row17\" >which includes various [PARENT] for example</th>\n",
       "      <td id=\"T_0576a_row17_col0\" class=\"data row17 col0\" >0.017780</td>\n",
       "      <td id=\"T_0576a_row17_col1\" class=\"data row17 col1\" >0.012290</td>\n",
       "      <td id=\"T_0576a_row17_col2\" class=\"data row17 col2\" >0.006000</td>\n",
       "      <td id=\"T_0576a_row17_col3\" class=\"data row17 col3\" >0.009670</td>\n",
       "      <td id=\"T_0576a_row17_col4\" class=\"data row17 col4\" >0.010640</td>\n",
       "      <td id=\"T_0576a_row17_col5\" class=\"data row17 col5\" >0.015070</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_0576a_level0_row18\" class=\"row_heading level0 row18\" >There are a lot of [PARENT] here for example</th>\n",
       "      <td id=\"T_0576a_row18_col0\" class=\"data row18 col0\" >0.001220</td>\n",
       "      <td id=\"T_0576a_row18_col1\" class=\"data row18 col1\" >0.000570</td>\n",
       "      <td id=\"T_0576a_row18_col2\" class=\"data row18 col2\" >0.000670</td>\n",
       "      <td id=\"T_0576a_row18_col3\" class=\"data row18 col3\" >0.000220</td>\n",
       "      <td id=\"T_0576a_row18_col4\" class=\"data row18 col4\" >0.000270</td>\n",
       "      <td id=\"T_0576a_row18_col5\" class=\"data row18 col5\" >0.000980</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_0576a_level0_row19\" class=\"row_heading level0 row19\" >[PARENT] e.g.</th>\n",
       "      <td id=\"T_0576a_row19_col0\" class=\"data row19 col0\" >0.055130</td>\n",
       "      <td id=\"T_0576a_row19_col1\" class=\"data row19 col1\" >0.034560</td>\n",
       "      <td id=\"T_0576a_row19_col2\" class=\"data row19 col2\" >0.029330</td>\n",
       "      <td id=\"T_0576a_row19_col3\" class=\"data row19 col3\" >0.027110</td>\n",
       "      <td id=\"T_0576a_row19_col4\" class=\"data row19 col4\" >0.033510</td>\n",
       "      <td id=\"T_0576a_row19_col5\" class=\"data row19 col5\" >0.038050</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_0576a_level0_row20\" class=\"row_heading level0 row20\" >[PARENT] like</th>\n",
       "      <td id=\"T_0576a_row20_col0\" class=\"data row20 col0\" >0.031820</td>\n",
       "      <td id=\"T_0576a_row20_col1\" class=\"data row20 col1\" >0.023450</td>\n",
       "      <td id=\"T_0576a_row20_col2\" class=\"data row20 col2\" >0.014000</td>\n",
       "      <td id=\"T_0576a_row20_col3\" class=\"data row20 col3\" >0.014330</td>\n",
       "      <td id=\"T_0576a_row20_col4\" class=\"data row20 col4\" >0.020230</td>\n",
       "      <td id=\"T_0576a_row20_col5\" class=\"data row20 col5\" >0.030060</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_0576a_level0_row21\" class=\"row_heading level0 row21\" >[PARENT] especially</th>\n",
       "      <td id=\"T_0576a_row21_col0\" class=\"data row21 col0\" >0.027980</td>\n",
       "      <td id=\"T_0576a_row21_col1\" class=\"data row21 col1\" >0.019120</td>\n",
       "      <td id=\"T_0576a_row21_col2\" class=\"data row21 col2\" >0.010000</td>\n",
       "      <td id=\"T_0576a_row21_col3\" class=\"data row21 col3\" >0.017110</td>\n",
       "      <td id=\"T_0576a_row21_col4\" class=\"data row21 col4\" >0.020000</td>\n",
       "      <td id=\"T_0576a_row21_col5\" class=\"data row21 col5\" >0.020840</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_0576a_level0_row22\" class=\"row_heading level0 row22\" >[PARENT] for example</th>\n",
       "      <td id=\"T_0576a_row22_col0\" class=\"data row22 col0\" >0.052770</td>\n",
       "      <td id=\"T_0576a_row22_col1\" class=\"data row22 col1\" >0.026530</td>\n",
       "      <td id=\"T_0576a_row22_col2\" class=\"data row22 col2\" >0.038000</td>\n",
       "      <td id=\"T_0576a_row22_col3\" class=\"data row22 col3\" >0.025560</td>\n",
       "      <td id=\"T_0576a_row22_col4\" class=\"data row22 col4\" >0.026590</td>\n",
       "      <td id=\"T_0576a_row22_col5\" class=\"data row22 col5\" >0.025540</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_0576a_level0_row23\" class=\"row_heading level0 row23\" >[PARENT] for instance</th>\n",
       "      <td id=\"T_0576a_row23_col0\" class=\"data row23 col0\" >0.055280</td>\n",
       "      <td id=\"T_0576a_row23_col1\" class=\"data row23 col1\" >0.028920</td>\n",
       "      <td id=\"T_0576a_row23_col2\" class=\"data row23 col2\" >0.041330</td>\n",
       "      <td id=\"T_0576a_row23_col3\" class=\"data row23 col3\" >0.028000</td>\n",
       "      <td id=\"T_0576a_row23_col4\" class=\"data row23 col4\" >0.027960</td>\n",
       "      <td id=\"T_0576a_row23_col5\" class=\"data row23 col5\" >0.028210</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n"
      ],
      "text/plain": [
       "<pandas.io.formats.style.Styler at 0x7efa35e86050>"
      ]
     },
     "execution_count": 136,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def highlight_max(s, props=''):\n",
    "    return np.where(s == np.nanmax(s.values), props, '')\n",
    "\n",
    "def highlight_min(s, props=''):\n",
    "    return np.where(s == np.nanmin(s.values), props, '')\n",
    "\n",
    "metrics_table2 = metrics_table.style.apply(highlight_max, \n",
    "                                          props='color:white; background-color:#1FC29D', axis=0).apply(highlight_min, props='color:white; background-color:#FF5555', axis=0)\n",
    "metrics_table2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62f85358-d9c9-4eac-828d-3d323c5133c6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
