{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e4946738-b2f4-498b-b3e1-5232b71a9504",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fri Mar 31 17:31:01 2023       \n",
      "+-----------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 510.108.03   Driver Version: 510.108.03   CUDA Version: 11.6     |\n",
      "|-------------------------------+----------------------+----------------------+\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                               |                      |               MIG M. |\n",
      "|===============================+======================+======================|\n",
      "|   0  NVIDIA GeForce ...  Off  | 00000000:1B:00.0 Off |                  Off |\n",
      "|  0%   39C    P8    26W / 460W |      8MiB / 24564MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   1  NVIDIA GeForce ...  Off  | 00000000:1C:00.0 Off |                  N/A |\n",
      "| 27%   34C    P8    14W / 260W |   9065MiB / 11264MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   2  NVIDIA GeForce ...  Off  | 00000000:1E:00.0 Off |                  N/A |\n",
      "| 27%   35C    P8    10W / 260W |      8MiB / 11264MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   3  NVIDIA GeForce ...  Off  | 00000000:3F:00.0 Off |                  Off |\n",
      "|  0%   32C    P8    19W / 460W |      8MiB / 24564MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   4  NVIDIA GeForce ...  Off  | 00000000:40:00.0 Off |                  Off |\n",
      "|  0%   36C    P8    27W / 460W |   8082MiB / 24564MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   5  NVIDIA GeForce ...  Off  | 00000000:5E:00.0 Off |                  Off |\n",
      "|  0%   35C    P8    25W / 460W |      8MiB / 24564MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "                                                                               \n",
      "+-----------------------------------------------------------------------------+\n",
      "| Processes:                                                                  |\n",
      "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
      "|        ID   ID                                                   Usage      |\n",
      "|=============================================================================|\n",
      "+-----------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7b6cc31c-642d-428e-94d7-8c41519c57fb",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting torch\n",
      "  Downloading torch-2.0.0-cp310-cp310-manylinux1_x86_64.whl (619.9 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m619.9/619.9 MB\u001b[0m \u001b[31m4.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting nvidia-cublas-cu11==11.10.3.66\n",
      "  Downloading nvidia_cublas_cu11-11.10.3.66-py3-none-manylinux1_x86_64.whl (317.1 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m317.1/317.1 MB\u001b[0m \u001b[31m7.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting nvidia-cusolver-cu11==11.4.0.1\n",
      "  Downloading nvidia_cusolver_cu11-11.4.0.1-2-py3-none-manylinux1_x86_64.whl (102.6 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m102.6/102.6 MB\u001b[0m \u001b[31m13.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting triton==2.0.0\n",
      "  Downloading triton-2.0.0-1-cp310-cp310-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (63.3 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m63.3/63.3 MB\u001b[0m \u001b[31m16.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting nvidia-cusparse-cu11==11.7.4.91\n",
      "  Downloading nvidia_cusparse_cu11-11.7.4.91-py3-none-manylinux1_x86_64.whl (173.2 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m173.2/173.2 MB\u001b[0m \u001b[31m10.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting nvidia-cudnn-cu11==8.5.0.96\n",
      "  Downloading nvidia_cudnn_cu11-8.5.0.96-2-py3-none-manylinux1_x86_64.whl (557.1 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m557.1/557.1 MB\u001b[0m \u001b[31m5.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting filelock\n",
      "  Downloading filelock-3.10.7-py3-none-any.whl (10 kB)\n",
      "Requirement already satisfied: typing-extensions in /opt/conda/lib/python3.10/site-packages (from torch) (4.4.0)\n",
      "Collecting nvidia-cufft-cu11==10.9.0.58\n",
      "  Downloading nvidia_cufft_cu11-10.9.0.58-py3-none-manylinux1_x86_64.whl (168.4 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m168.4/168.4 MB\u001b[0m \u001b[31m11.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting nvidia-nvtx-cu11==11.7.91\n",
      "  Downloading nvidia_nvtx_cu11-11.7.91-py3-none-manylinux1_x86_64.whl (98 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m98.6/98.6 kB\u001b[0m \u001b[31m44.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting nvidia-nccl-cu11==2.14.3\n",
      "  Downloading nvidia_nccl_cu11-2.14.3-py3-none-manylinux1_x86_64.whl (177.1 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m177.1/177.1 MB\u001b[0m \u001b[31m12.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch) (3.0)\n",
      "Requirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch) (1.11.1)\n",
      "Collecting nvidia-curand-cu11==10.2.10.91\n",
      "  Downloading nvidia_curand_cu11-10.2.10.91-py3-none-manylinux1_x86_64.whl (54.6 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m54.6/54.6 MB\u001b[0m \u001b[31m23.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting nvidia-cuda-runtime-cu11==11.7.99\n",
      "  Downloading nvidia_cuda_runtime_cu11-11.7.99-py3-none-manylinux1_x86_64.whl (849 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m849.3/849.3 kB\u001b[0m \u001b[31m48.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting nvidia-cuda-cupti-cu11==11.7.101\n",
      "  Downloading nvidia_cuda_cupti_cu11-11.7.101-py3-none-manylinux1_x86_64.whl (11.8 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m11.8/11.8 MB\u001b[0m \u001b[31m34.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting nvidia-cuda-nvrtc-cu11==11.7.99\n",
      "  Downloading nvidia_cuda_nvrtc_cu11-11.7.99-2-py3-none-manylinux1_x86_64.whl (21.0 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.0/21.0 MB\u001b[0m \u001b[31m29.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch) (3.1.2)\n",
      "Requirement already satisfied: setuptools in /opt/conda/lib/python3.10/site-packages (from nvidia-cublas-cu11==11.10.3.66->torch) (66.1.1)\n",
      "Requirement already satisfied: wheel in /opt/conda/lib/python3.10/site-packages (from nvidia-cublas-cu11==11.10.3.66->torch) (0.38.4)\n",
      "Collecting lit\n",
      "  Downloading lit-16.0.0.tar.gz (144 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m145.0/145.0 kB\u001b[0m \u001b[31m47.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hCollecting cmake\n",
      "  Downloading cmake-3.26.1-py2.py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (24.0 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.0/24.0 MB\u001b[0m \u001b[31m28.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch) (2.1.2)\n",
      "Requirement already satisfied: mpmath>=0.19 in /opt/conda/lib/python3.10/site-packages (from sympy->torch) (1.2.1)\n",
      "Building wheels for collected packages: lit\n",
      "  Building wheel for lit (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for lit: filename=lit-16.0.0-py3-none-any.whl size=93582 sha256=214925c16ae7848f52dcb2f234f11e4762db187191bf55e31cfcaaa124f27cf5\n",
      "  Stored in directory: /home/jovyan/.cache/pip/wheels/e5/7f/66/8bfd6d52625bf85b29fa507f79fbc3f5cb4bb72eae40318074\n",
      "Successfully built lit\n",
      "Installing collected packages: lit, cmake, nvidia-nvtx-cu11, nvidia-nccl-cu11, nvidia-cusparse-cu11, nvidia-curand-cu11, nvidia-cufft-cu11, nvidia-cuda-runtime-cu11, nvidia-cuda-nvrtc-cu11, nvidia-cuda-cupti-cu11, nvidia-cublas-cu11, filelock, nvidia-cusolver-cu11, nvidia-cudnn-cu11, triton, torch\n",
      "Successfully installed cmake-3.26.1 filelock-3.10.7 lit-16.0.0 nvidia-cublas-cu11-11.10.3.66 nvidia-cuda-cupti-cu11-11.7.101 nvidia-cuda-nvrtc-cu11-11.7.99 nvidia-cuda-runtime-cu11-11.7.99 nvidia-cudnn-cu11-8.5.0.96 nvidia-cufft-cu11-10.9.0.58 nvidia-curand-cu11-10.2.10.91 nvidia-cusolver-cu11-11.4.0.1 nvidia-cusparse-cu11-11.7.4.91 nvidia-nccl-cu11-2.14.3 nvidia-nvtx-cu11-11.7.91 torch-2.0.0 triton-2.0.0\n",
      "Collecting transformers\n",
      "  Downloading transformers-4.27.4-py3-none-any.whl (6.8 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.8/6.8 MB\u001b[0m \u001b[31m17.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.10/site-packages (from transformers) (1.23.5)\n",
      "Requirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from transformers) (3.10.7)\n",
      "Requirement already satisfied: tqdm>=4.27 in /opt/conda/lib/python3.10/site-packages (from transformers) (4.64.1)\n",
      "Collecting tokenizers!=0.11.3,<0.14,>=0.11.1\n",
      "  Downloading tokenizers-0.13.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.6 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.6/7.6 MB\u001b[0m \u001b[31m25.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0mm\n",
      "\u001b[?25hRequirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.10/site-packages (from transformers) (6.0)\n",
      "Collecting regex!=2019.12.17\n",
      "  Downloading regex-2023.3.23-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (769 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m769.6/769.6 kB\u001b[0m \u001b[31m70.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from transformers) (2.28.2)\n",
      "Collecting huggingface-hub<1.0,>=0.11.0\n",
      "  Downloading huggingface_hub-0.13.3-py3-none-any.whl (199 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m199.8/199.8 kB\u001b[0m \u001b[31m67.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.10/site-packages (from transformers) (23.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.11.0->transformers) (4.4.0)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (1.26.14)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (2.1.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (3.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (2022.12.7)\n",
      "Installing collected packages: tokenizers, regex, huggingface-hub, transformers\n",
      "Successfully installed huggingface-hub-0.13.3 regex-2023.3.23 tokenizers-0.13.2 transformers-4.27.4\n"
     ]
    }
   ],
   "source": [
    "!pip install torch\n",
    "!pip install transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8c6b35a9-023c-4ff8-8739-bc978c1a4be0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "from transformers import T5ForConditionalGeneration, AutoTokenizer\n",
    "from transformers import AutoModelForSeq2SeqLM, AutoTokenizer, AutoConfig\n",
    "\n",
    "\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import Trainer, TrainingArguments\n",
    "from transformers.file_utils import cached_property\n",
    "from typing import Tuple\n",
    "from sklearn.model_selection import train_test_split\n",
    "import gc\n",
    "from tqdm.auto import tqdm, trange\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "from sklearn.utils import shuffle\n",
    "\n",
    "from collections import Counter\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9a913f3-d530-4177-b1a6-97a87397db80",
   "metadata": {},
   "source": [
    "# model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "58a161d0-9df2-4e9b-9eaa-87c57e07636c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from get_cuda_device import get_cuda_command\n",
    "\n",
    "get_cuda_command('cuda:0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8a26c278-e3ee-4084-97f6-dac02ab584b9",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU\n"
     ]
    }
   ],
   "source": [
    "import os \n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\n",
    "\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    device='cuda'\n",
    "    print('GPU')\n",
    "else:\n",
    "    device='cpu'\n",
    "    print('CPU')\n",
    "    \n",
    "    \n",
    "SEED = 0\n",
    "torch.manual_seed(SEED)\n",
    "torch.cuda.manual_seed(SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ec73cecd-abb8-4635-a184-430959e38258",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "276b44a5a1304ea4861846a6edd7ab32",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)lve/main/config.json:   0%|          | 0.00/1.21k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9dd0337b1af643b7813161fe8038b6ac",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading pytorch_model.bin:   0%|          | 0.00/2.95G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "549d4a31afe047a089d84cf22c2b869c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)neration_config.json:   0%|          | 0.00/147 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2f55cc6b24d149e19655ca26f5c09c3e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)ve/main/spiece.model:   0%|          | 0.00/792k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4d6967a26fb844cd9d0c3dfaea14d607",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)/main/tokenizer.json:   0%|          | 0.00/1.39M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/transformers/models/t5/tokenization_t5_fast.py:155: FutureWarning: This tokenizer was incorrectly instantiated with a model max length of 512 which will be corrected in Transformers v5.\n",
      "For now, this behavior is kept to avoid breaking backwards compatibility when padding/encoding with `truncation is True`.\n",
      "- Be aware that you SHOULD NOT rely on t5-large automatically truncating your input to 512 when padding/encoding.\n",
      "- If you want to encode/pad to sequences longer than 512 you can either instantiate this tokenizer with `model_max_length` or pass `max_length` when encoding/padding.\n",
      "- To avoid this warning, please instantiate this tokenizer with `model_max_length` set to your preferred value.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from transformers import TrainingArguments, Trainer\n",
    "\n",
    "model_checkpoint = \"t5-large\"\n",
    "model = T5ForConditionalGeneration.from_pretrained(model_checkpoint).to(device)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3171f4cc-6472-4797-883e-2cb95c4d0c52",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class PairsDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, x, y):\n",
    "        self.x = x\n",
    "        self.y = y\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        assert idx <= len(self.x['input_ids']), (idx, len(self.x['input_ids']))\n",
    "        item = {key: val[idx] for key, val in self.x.items()}\n",
    "        item['decoder_attention_mask'] = self.y['attention_mask'][idx]\n",
    "        item['labels'] = self.y['input_ids'][idx]\n",
    "        return item\n",
    "    \n",
    "    @property\n",
    "    def n(self):\n",
    "        return len(self.x['input_ids'])\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "87b3137b-4300-4c74-9822-6aaefce55e37",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from typing import List, Dict, Union\n",
    "\n",
    "class DataCollatorWithPadding:\n",
    "    def __init__(self, tokenizer):\n",
    "        self.tokenizer = tokenizer\n",
    "\n",
    "    def __call__(self, features: List[Dict[str, Union[List[int], torch.Tensor]]]) -> Dict[str, torch.Tensor]:\n",
    "        batch = self.tokenizer.pad(\n",
    "            features,\n",
    "            padding=True,\n",
    "        )\n",
    "        ybatch = self.tokenizer.pad(\n",
    "            {'input_ids': batch['labels'], 'attention_mask': batch['decoder_attention_mask']},\n",
    "            padding=True,\n",
    "        ) \n",
    "        batch['labels'] = ybatch['input_ids']\n",
    "        batch['decoder_attention_mask'] = ybatch['attention_mask']\n",
    "        \n",
    "        return {k: torch.tensor(v) for k, v in batch.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7ef1229c-d458-413c-acc9-2fb0221248b9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def cleanup():\n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()\n",
    "    \n",
    "cleanup()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ce916472-8e8c-4436-8028-9fc1ec6ac61a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def evaluate_model(model, test_dataloader):\n",
    "    num = 0\n",
    "    den = 0\n",
    "\n",
    "    for batch in test_dataloader:\n",
    "        with torch.no_grad():\n",
    "            loss = model(**{k: v.to(model.device) for k, v in batch.items()}).loss\n",
    "            num += len(batch) * loss.item()\n",
    "            den += len(batch)\n",
    "    val_loss = num / den\n",
    "    return val_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6c063e4-fa59-487c-96a0-d9a8cf7f35f5",
   "metadata": {},
   "source": [
    "# **Read data SemEval2018-Task9**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "43c5909a-ed84-4545-a355-963f054facdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = os.getcwd().replace('prefix', '')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c2344944-6187-49b1-bcb9-0f1b98c505ce",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>term</th>\n",
       "      <th>relation</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>blackfly</td>\n",
       "      <td>Concept</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Turonian</td>\n",
       "      <td>Entity</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>abhorrence</td>\n",
       "      <td>Concept</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>tropical storm</td>\n",
       "      <td>Concept</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>militarization</td>\n",
       "      <td>Concept</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             term relation\n",
       "0        blackfly  Concept\n",
       "1        Turonian   Entity\n",
       "2      abhorrence  Concept\n",
       "3  tropical storm  Concept\n",
       "4  militarization  Concept"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "path_data_en = path+\"SemEval2018-Task9/training/data/1A.english.training.data.txt\"\n",
    "path_gold_en = path+\"SemEval2018-Task9/training/gold/1A.english.training.gold.txt\"\n",
    "\n",
    "train_data_en_data = pd.read_csv(path_data_en, header=None, sep=\"\\t\", names=['term', 'relation'])\n",
    "train_gold_en_data = pd.read_csv(path_gold_en, header=None, names=['hypernym'])\n",
    "\n",
    "train_data_en_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "129edda7-bf42-43e4-9fd0-7e55aa8bc714",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "path_test_data_en = path+\"SemEval2018-Task9/test/data/1A.english.test.data.txt\"\n",
    "path_test_gold_en = path+\"SemEval2018-Task9/test/gold/1A.english.test.gold.txt\"\n",
    "\n",
    "test_data_en_data = pd.read_csv(path_test_data_en, header=None, sep=\"\\t\", names=['term', 'relation'])\n",
    "test_gold_en_data = pd.read_csv(path_test_gold_en, header=None, names=['hypernym'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "a7330473-9221-4861-8827-6502dd8e8ff1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def embading_find_hyponyms_uppercase(train_features, train_target, test_features, test_target):\n",
    "    \n",
    "    train_data_en = train_features.copy()\n",
    "    train_data_en = 'find hypernyms for hyponym: ' + train_data_en.term + ' , target:'\n",
    "    print(train_data_en.head())\n",
    "\n",
    "    train_gold_en = train_target.copy()\n",
    "    train_gold_en = train_gold_en.hypernym.str.split('\\t').str.join(', ')\n",
    "    print(train_gold_en.head())\n",
    "\n",
    "    test_data_en = test_features.copy()\n",
    "    test_data_en = 'find hypernyms for hyponym: ' + test_data_en.term + ' , target:'\n",
    "\n",
    "    test_gold_en = test_target.copy()\n",
    "    test_gold_en = test_gold_en.hypernym.str.split('\\t').str.join(', ')\n",
    "    \n",
    "    return train_data_en, train_gold_en, test_data_en, test_gold_en"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "25f78662-6206-48f6-8843-051847ea8f76",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0       find hypernyms for hyponym: blackfly , target:\n",
      "1       find hypernyms for hyponym: Turonian , target:\n",
      "2     find hypernyms for hyponym: abhorrence , target:\n",
      "3    find hypernyms for hyponym: tropical storm , t...\n",
      "4    find hypernyms for hyponym: militarization , t...\n",
      "Name: term, dtype: object\n",
      "0                           homopterous insect, insect\n",
      "1    technical specification, geologic timescale, p...\n",
      "2                      distaste, hatred, hate, disgust\n",
      "3    atmosphere, windstorm, violent storm, air curr...\n",
      "4                                       social control\n",
      "Name: hypernym, dtype: object\n"
     ]
    }
   ],
   "source": [
    "train_data_en, train_gold_en, test_data_en, test_gold_en = embading_find_hyponyms_uppercase(train_data_en_data, \n",
    "                                                                                 train_gold_en_data, \n",
    "                                                                                 test_data_en_data, \n",
    "                                                                                 test_gold_en_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b52fad45-8557-4569-85e9-84a9c8de4d1c",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Model training embading_find_hyponyms_uppercase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "f9a20d05-cc19-4151-88b2-e044f4650bc2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_dataset = PairsDataset(tokenizer(train_data_en.tolist()), tokenizer(train_gold_en.tolist()))\n",
    "test_dataset = PairsDataset(tokenizer(test_data_en.tolist()), tokenizer(test_gold_en.tolist()))\n",
    "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n",
    "\n",
    "args = TrainingArguments(output_dir=\"t5-finetuned-large\", \n",
    "                         num_train_epochs=16, \n",
    "                         per_device_train_batch_size=16, save_steps=10000)\n",
    "\n",
    "trainer = Trainer(\n",
    "    model = model,\n",
    "    args = args,\n",
    "    train_dataset = train_dataset,\n",
    "    eval_dataset = test_dataset,\n",
    "    tokenizer = tokenizer,\n",
    "    data_collator = data_collator\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "00a494a6-1f75-4c1e-977d-481d8b857707",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/transformers/optimization.py:391: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "You're using a T5TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1504' max='1504' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1504/1504 08:37, Epoch 16/16]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>0.967800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.643600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>0.574500</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=1504, training_loss=0.7283922998194999, metrics={'train_runtime': 518.6368, 'train_samples_per_second': 46.275, 'train_steps_per_second': 2.9, 'total_flos': 2255860881408000.0, 'train_loss': 0.7283922998194999, 'epoch': 16.0})"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "d1d24206-3655-468e-b961-964bde0bc939",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "Fri Mar 31 15:35:55 2023       \n",
      "+-----------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 510.108.03   Driver Version: 510.108.03   CUDA Version: 11.6     |\n",
      "|-------------------------------+----------------------+----------------------+\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                               |                      |               MIG M. |\n",
      "|===============================+======================+======================|\n",
      "|   0  NVIDIA GeForce ...  Off  | 00000000:1B:00.0 Off |                  Off |\n",
      "|100%   63C    P2   312W / 460W |  21307MiB / 24564MiB |     14%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   1  NVIDIA GeForce ...  Off  | 00000000:1C:00.0 Off |                  N/A |\n",
      "| 27%   35C    P8    15W / 260W |   9065MiB / 11264MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   2  NVIDIA GeForce ...  Off  | 00000000:1E:00.0 Off |                  N/A |\n",
      "| 26%   36C    P8    11W / 260W |   9743MiB / 11264MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   3  NVIDIA GeForce ...  Off  | 00000000:3F:00.0 Off |                  Off |\n",
      "|  0%   32C    P8    18W / 460W |      8MiB / 24564MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   4  NVIDIA GeForce ...  Off  | 00000000:40:00.0 Off |                  Off |\n",
      "|  0%   37C    P8    29W / 460W |   8082MiB / 24564MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   5  NVIDIA GeForce ...  Off  | 00000000:5E:00.0 Off |                  Off |\n",
      "|  0%   35C    P8    25W / 460W |      8MiB / 24564MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "                                                                               \n",
      "+-----------------------------------------------------------------------------+\n",
      "| Processes:                                                                  |\n",
      "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
      "|        ID   ID                                                   Usage      |\n",
      "|=============================================================================|\n",
      "+-----------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eaec1fb2-d985-4533-9bf0-e6eb06f31f4f",
   "metadata": {},
   "source": [
    "# EVALUATION "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "94edd7e6-385b-4747-99a1-b98bba45590b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def predict(test_data_en, test_gold_en):\n",
    "    test_pred_en=[]\n",
    "    for i_2, j_2 in tqdm(zip(test_data_en.tolist(), test_gold_en.tolist())):\n",
    "            input_ids = tokenizer.encode(i_2, return_tensors=\"pt\")\n",
    "            output_batch = model.generate(input_ids.cuda(), no_repeat_ngram_size=2, max_new_tokens=2048, \n",
    "                                          num_return_sequences=50, num_beams=50, early_stopping=True, \n",
    "                                          num_beam_groups=5, \n",
    "                                          diversity_penalty=1.0)\n",
    "            decoded_list = []\n",
    "            for outputs in output_batch:\n",
    "                decoded = tokenizer.decode(outputs, skip_special_tokens=True).split(\", \")\n",
    "                decoded_list.extend(decoded)\n",
    "\n",
    "            sorted_predicted_answer = [i[0] for i in Counter(decoded_list).most_common()]\n",
    "\n",
    "            test_pred_en.append(sorted_predicted_answer)\n",
    "    return test_pred_en\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "a03db437-d4cc-4746-92ce-ac7d5ce0716e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1500it [09:21,  2.67it/s]\n"
     ]
    }
   ],
   "source": [
    "test_pred_en = predict(test_data_en, test_gold_en)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "edb90fc4-167a-400e-b106-a07900dd126e",
   "metadata": {},
   "outputs": [],
   "source": [
    "name  = 'find_pred.txt'\n",
    "\n",
    "find_pred = []\n",
    "for i in test_pred_en:\n",
    "    find_pred.append('\\t'.join(i))\n",
    "\n",
    "\n",
    "test_pred_en_df = pd.DataFrame(find_pred)\n",
    "test_pred_en_df.to_csv(name, header=None, index=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5747f0c9-f258-45ee-8627-b208e3ba196e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def answers(str_ans):\n",
    "    columns_name = []\n",
    "    values = []\n",
    "    for ind, metrics in enumerate(str_ans.split('\\n')):\n",
    "        if ind == 6:\n",
    "            _name = 'prefix'\n",
    "            number = metrics\n",
    "        else:\n",
    "            _name, number = metrics.split(' ')\n",
    "            number = round(float(number), 5)\n",
    "            _name = _name[:-1]\n",
    "        \n",
    "        columns_name.append(_name)\n",
    "        values.append([number])\n",
    "        \n",
    "    \n",
    "        \n",
    "    df = pd.DataFrame(values).T\n",
    "    df.columns = columns_name\n",
    "    df.set_index('prefix', inplace=True)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "69430b9c-53e7-4f23-9169-c0bfe2f0f4c1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>MRR</th>\n",
       "      <th>MAP</th>\n",
       "      <th>P@1</th>\n",
       "      <th>P@3</th>\n",
       "      <th>P@5</th>\n",
       "      <th>P@15</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>prefix</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>find hypernyms for hyponym: [CHILD] , target: [PARENTS]</th>\n",
       "      <td>0.41024</td>\n",
       "      <td>0.2557</td>\n",
       "      <td>0.32267</td>\n",
       "      <td>0.22956</td>\n",
       "      <td>0.22899</td>\n",
       "      <td>0.27718</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                        MRR     MAP      P@1  \\\n",
       "prefix                                                                         \n",
       "find hypernyms for hyponym: [CHILD] , target: [...  0.41024  0.2557  0.32267   \n",
       "\n",
       "                                                        P@3      P@5     P@15  \n",
       "prefix                                                                         \n",
       "find hypernyms for hyponym: [CHILD] , target: [...  0.22956  0.22899  0.27718  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from contextlib import redirect_stdout\n",
    "import io\n",
    "from tqdm import tqdm\n",
    "from IPython.display import clear_output\n",
    "\n",
    "f = io.StringIO()\n",
    "with redirect_stdout(f):\n",
    "    !python scorer.py /home/jovyan/work/SemEval2018-Task9/test/gold/1A.english.test.gold.txt find_pred.txt\n",
    "\n",
    "\n",
    "_std_out = f.getvalue()\n",
    "_std_out = _std_out  + 'find hypernyms for hyponym: [CHILD] , target: [PARENTS]'\n",
    "embading_find_hyponyms_uppercase_table = answers(_std_out)\n",
    "embading_find_hyponyms_uppercase_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "c019a190-e23a-431d-86d6-e6e0f12cf915",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# name  = 'test_pred_en_2v_14_02.txt'\n",
    "\n",
    "# test_pred_en_df = []\n",
    "# for i in test_pred_en:\n",
    "#     test_pred_en_df.append('\\t'.join(i))\n",
    "\n",
    "\n",
    "# test_pred_en_df = pd.DataFrame(test_pred_en_df)\n",
    "# test_pred_en_df.to_csv(name, header=None, index=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "2314c2f4-6257-45cd-899d-0fdd7fca0859",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "cleanup()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "ccaff81d-61f8-4da3-bb39-4f03bad88325",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "del trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "21cdba36-3c07-4a19-8bc4-33fc7d7fc9fe",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "Fri Mar 31 15:57:28 2023       \n",
      "+-----------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 510.108.03   Driver Version: 510.108.03   CUDA Version: 11.6     |\n",
      "|-------------------------------+----------------------+----------------------+\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                               |                      |               MIG M. |\n",
      "|===============================+======================+======================|\n",
      "|   0  NVIDIA GeForce ...  Off  | 00000000:1B:00.0 Off |                  Off |\n",
      "|  0%   41C    P2   115W / 460W |   9665MiB / 24564MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   1  NVIDIA GeForce ...  Off  | 00000000:1C:00.0 Off |                  N/A |\n",
      "| 27%   33C    P8    14W / 260W |   9065MiB / 11264MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   2  NVIDIA GeForce ...  Off  | 00000000:1E:00.0 Off |                  N/A |\n",
      "| 27%   33C    P8    11W / 260W |   9743MiB / 11264MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   3  NVIDIA GeForce ...  Off  | 00000000:3F:00.0 Off |                  Off |\n",
      "|  0%   32C    P8    18W / 460W |      8MiB / 24564MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   4  NVIDIA GeForce ...  Off  | 00000000:40:00.0 Off |                  Off |\n",
      "|  0%   36C    P8    28W / 460W |   8082MiB / 24564MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   5  NVIDIA GeForce ...  Off  | 00000000:5E:00.0 Off |                  Off |\n",
      "|  0%   35C    P8    25W / 460W |      8MiB / 24564MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "                                                                               \n",
      "+-----------------------------------------------------------------------------+\n",
      "| Processes:                                                                  |\n",
      "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
      "|        ID   ID                                                   Usage      |\n",
      "|=============================================================================|\n",
      "+-----------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ce0ebf9-3281-49f4-ba93-bb7715d7d044",
   "metadata": {},
   "source": [
    "# Standard emabdding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "89bd88e1-1db4-4ab3-933c-68bb16576b82",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0          hyponym: blackfly | hypernyms:\n",
      "1          hyponym: Turonian | hypernyms:\n",
      "2        hyponym: abhorrence | hypernyms:\n",
      "3    hyponym: tropical storm | hypernyms:\n",
      "4    hyponym: militarization | hypernyms:\n",
      "Name: term, dtype: object\n",
      "0                           homopterous insect, insect\n",
      "1    technical specification, geologic timescale, p...\n",
      "2                      distaste, hatred, hate, disgust\n",
      "3    atmosphere, windstorm, violent storm, air curr...\n",
      "4                                       social control\n",
      "Name: hypernym, dtype: object\n",
      "0    hyponym: maliciousness | hypernyms:\n",
      "1          hyponym: buckler | hypernyms:\n",
      "2        hyponym: spelunker | hypernyms:\n",
      "3     hyponym: quo warranto | hypernyms:\n",
      "4     hyponym: Jeff Francis | hypernyms:\n",
      "Name: term, dtype: object\n",
      "0       malevolence, distaste, hatred, hate, malignity\n",
      "1                                           body armor\n",
      "2                    exploration, adventurer, explorer\n",
      "3    proceedings, legal proceedings, proceeding, du...\n",
      "4               thrower, baseball player, jock, person\n",
      "Name: hypernym, dtype: object\n"
     ]
    }
   ],
   "source": [
    "def standard_preprocessing(train_features, train_target, test_features, test_target):\n",
    "    \n",
    "    train_data_en = train_features.copy()\n",
    "    train_data_en = 'hyponym: ' + train_data_en.term + ' | hypernyms:'\n",
    "    print(train_data_en.head())\n",
    "\n",
    "    train_gold_en = train_target.copy()\n",
    "    train_gold_en = train_gold_en.hypernym.str.split('\\t').str.join(', ')\n",
    "    print(train_gold_en.head())\n",
    "    \n",
    "    test_data_en = test_features.copy()\n",
    "    test_data_en = 'hyponym: ' + test_data_en.term + ' | hypernyms:'\n",
    "    print(test_data_en.head())\n",
    "\n",
    "    test_gold_en = test_target.copy()\n",
    "    test_gold_en = test_gold_en.hypernym.str.split('\\t').str.join(', ')\n",
    "    print(test_gold_en.head())\n",
    "    \n",
    "    return train_data_en, train_gold_en, test_data_en, test_gold_en\n",
    "\n",
    "train_data_en, train_gold_en, test_data_en, test_gold_en = standard_preprocessing(train_data_en_data, \n",
    "                                                                                 train_gold_en_data, \n",
    "                                                                                 test_data_en_data, \n",
    "                                                                                 test_gold_en_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2dcae852-322f-4282-b3b1-6a0ae0d21f86",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_dataset = PairsDataset(tokenizer(train_data_en.tolist()), tokenizer(train_gold_en.tolist()))\n",
    "test_dataset = PairsDataset(tokenizer(test_data_en.tolist()), tokenizer(test_gold_en.tolist()))\n",
    "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n",
    "\n",
    "args = TrainingArguments(output_dir=\"t5-finetuned-large\", \n",
    "                         num_train_epochs=16, \n",
    "                         per_device_train_batch_size=16, save_steps=10000)\n",
    "\n",
    "trainer = Trainer(\n",
    "    model = model,\n",
    "    args = args,\n",
    "    train_dataset = train_dataset,\n",
    "    eval_dataset = test_dataset,\n",
    "    tokenizer = tokenizer,\n",
    "    data_collator = data_collator\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "6bb91978-40bb-46fe-881b-7c10b81bc8e1",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/transformers/optimization.py:391: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1504' max='1504' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1504/1504 08:34, Epoch 16/16]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>0.542500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.463000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>0.416900</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=1504, training_loss=0.4741376611463567, metrics={'train_runtime': 514.9895, 'train_samples_per_second': 46.603, 'train_steps_per_second': 2.92, 'total_flos': 1849914513408000.0, 'train_loss': 0.4741376611463567, 'epoch': 16.0})"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "50e1c5cf-4d9e-421e-a2a3-56dddbcbe604",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1500it [14:04,  1.78it/s]\n"
     ]
    }
   ],
   "source": [
    "test_pred_en_standard = predict(test_data_en, test_gold_en)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "0ed5a0f8-bd2c-4b40-9871-7b02dd29538b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "name  = 'test_pred_en_standard.txt'\n",
    "\n",
    "test_pred_en_df = []\n",
    "for i in test_pred_en_standard:\n",
    "    test_pred_en_df.append('\\t'.join(i))\n",
    "\n",
    "\n",
    "test_pred_en_df = pd.DataFrame(test_pred_en_df)\n",
    "test_pred_en_df.to_csv(name, header=None, index=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "1fb7fc10-7e71-4241-8782-83fd21a96537",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "MRR: 0.3940902930402933\n",
      "MAP: 0.2506139995806665\n",
      "P@1: 0.30866666666666664\n",
      "P@3: 0.22044444444444422\n",
      "P@5: 0.21683333333333213\n",
      "P@15: 0.2820797202797196\n"
     ]
    }
   ],
   "source": [
    "!python scorer.py /home/jovyan/work/SemEval2018-Task9/test/gold/1A.english.test.gold.txt test_pred_en_standard.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0a0e53d3-854d-48f7-a958-cf61a44f1ce9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>MRR</th>\n",
       "      <th>MAP</th>\n",
       "      <th>P@1</th>\n",
       "      <th>P@3</th>\n",
       "      <th>P@5</th>\n",
       "      <th>P@15</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>prefix</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>hyponym: [CHILD] | hypernyms: [PARENTS]</th>\n",
       "      <td>0.39409</td>\n",
       "      <td>0.25061</td>\n",
       "      <td>0.30867</td>\n",
       "      <td>0.22044</td>\n",
       "      <td>0.21683</td>\n",
       "      <td>0.28208</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             MRR      MAP      P@1      P@3  \\\n",
       "prefix                                                                        \n",
       "hyponym: [CHILD] | hypernyms: [PARENTS]  0.39409  0.25061  0.30867  0.22044   \n",
       "\n",
       "                                             P@5     P@15  \n",
       "prefix                                                     \n",
       "hyponym: [CHILD] | hypernyms: [PARENTS]  0.21683  0.28208  "
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f = io.StringIO()\n",
    "with redirect_stdout(f):\n",
    "    !python scorer.py /home/jovyan/work/SemEval2018-Task9/test/gold/1A.english.test.gold.txt test_pred_en_standard.txt\n",
    "\n",
    "\n",
    "_std_out = f.getvalue()\n",
    "_std_out = _std_out  + 'hyponym: [CHILD] | hypernyms: [PARENTS]'\n",
    "embading_standard_table = answers(_std_out)\n",
    "embading_standard_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "d2de5585-0bde-4aa5-89e2-73841107c929",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "cleanup()\n",
    "del trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "daa7895b-09c2-4708-8d31-28581d39024b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "Fri Mar 31 16:48:46 2023       \n",
      "+-----------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 510.108.03   Driver Version: 510.108.03   CUDA Version: 11.6     |\n",
      "|-------------------------------+----------------------+----------------------+\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                               |                      |               MIG M. |\n",
      "|===============================+======================+======================|\n",
      "|   0  NVIDIA GeForce ...  Off  | 00000000:1B:00.0 Off |                  Off |\n",
      "|  0%   37C    P2    66W / 460W |   9609MiB / 24564MiB |     15%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   1  NVIDIA GeForce ...  Off  | 00000000:1C:00.0 Off |                  N/A |\n",
      "| 27%   32C    P8    14W / 260W |   9065MiB / 11264MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   2  NVIDIA GeForce ...  Off  | 00000000:1E:00.0 Off |                  N/A |\n",
      "| 27%   32C    P8    10W / 260W |   9743MiB / 11264MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   3  NVIDIA GeForce ...  Off  | 00000000:3F:00.0 Off |                  Off |\n",
      "|  0%   31C    P8    18W / 460W |      8MiB / 24564MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   4  NVIDIA GeForce ...  Off  | 00000000:40:00.0 Off |                  Off |\n",
      "|  0%   35C    P8    28W / 460W |   8082MiB / 24564MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   5  NVIDIA GeForce ...  Off  | 00000000:5E:00.0 Off |                  Off |\n",
      "|  0%   34C    P8    25W / 460W |      8MiB / 24564MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "                                                                               \n",
      "+-----------------------------------------------------------------------------+\n",
      "| Processes:                                                                  |\n",
      "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
      "|        ID   ID                                                   Usage      |\n",
      "|=============================================================================|\n",
      "+-----------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1464e373-c21d-4dd1-85dd-e1cbcab882a8",
   "metadata": {
    "tags": []
   },
   "source": [
    "# QandA_preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "d50ef2b3-94f8-4052-90c6-54c14bceab75",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0          what are hypernyms for hyponym blackfly ?\n",
      "1          what are hypernyms for hyponym Turonian ?\n",
      "2        what are hypernyms for hyponym abhorrence ?\n",
      "3    what are hypernyms for hyponym tropical storm ?\n",
      "4    what are hypernyms for hyponym militarization ?\n",
      "Name: term, dtype: object\n",
      "0                           homopterous insect, insect\n",
      "1    technical specification, geologic timescale, p...\n",
      "2                      distaste, hatred, hate, disgust\n",
      "3    atmosphere, windstorm, violent storm, air curr...\n",
      "4                                       social control\n",
      "Name: hypernym, dtype: object\n",
      "0    what are hypernyms for hyponym maliciousness ?\n",
      "1          what are hypernyms for hyponym buckler ?\n",
      "2        what are hypernyms for hyponym spelunker ?\n",
      "3     what are hypernyms for hyponym quo warranto ?\n",
      "4     what are hypernyms for hyponym Jeff Francis ?\n",
      "Name: term, dtype: object\n",
      "0       malevolence, distaste, hatred, hate, malignity\n",
      "1                                           body armor\n",
      "2                    exploration, adventurer, explorer\n",
      "3    proceedings, legal proceedings, proceeding, du...\n",
      "4               thrower, baseball player, jock, person\n",
      "Name: hypernym, dtype: object\n"
     ]
    }
   ],
   "source": [
    "def QandA_preprocessing(train_features, train_target, test_features, test_target):\n",
    "    \n",
    "    train_data_en = train_features.copy()\n",
    "    train_data_en = 'what are hypernyms for hyponym ' + train_data_en.term + ' ?'\n",
    "    print(train_data_en.head())\n",
    "\n",
    "    train_gold_en = train_target.copy()\n",
    "    train_gold_en = train_gold_en.hypernym.str.split('\\t').str.join(', ')\n",
    "    print(train_gold_en.head())\n",
    "    \n",
    "    test_data_en = test_features.copy()\n",
    "    test_data_en = 'what are hypernyms for hyponym '  + test_data_en.term + ' ?'\n",
    "    print(test_data_en.head())\n",
    "\n",
    "    test_gold_en = test_target.copy()\n",
    "    test_gold_en = test_gold_en.hypernym.str.split('\\t').str.join(', ')\n",
    "    print(test_gold_en.head())\n",
    "    \n",
    "    return train_data_en, train_gold_en, test_data_en, test_gold_en\n",
    "\n",
    "train_data_en, train_gold_en, test_data_en, test_gold_en = QandA_preprocessing(train_data_en_data, \n",
    "                                                                                 train_gold_en_data, \n",
    "                                                                                 test_data_en_data, \n",
    "                                                                                 test_gold_en_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "1e899d83-3f83-4eff-b420-8f362f72b125",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_dataset = PairsDataset(tokenizer(train_data_en.tolist()), tokenizer(train_gold_en.tolist()))\n",
    "test_dataset = PairsDataset(tokenizer(test_data_en.tolist()), tokenizer(test_gold_en.tolist()))\n",
    "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n",
    "\n",
    "args = TrainingArguments(output_dir=\"t5-finetuned-large\", \n",
    "                         num_train_epochs=16, \n",
    "                         per_device_train_batch_size=16, save_steps=10000)\n",
    "\n",
    "trainer = Trainer(\n",
    "    model = model,\n",
    "    args = args,\n",
    "    train_dataset = train_dataset,\n",
    "    eval_dataset = test_dataset,\n",
    "    tokenizer = tokenizer,\n",
    "    data_collator = data_collator\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "a991d752-60cb-48fd-8682-459dc2b5389b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/transformers/optimization.py:391: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1504' max='1504' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1504/1504 08:34, Epoch 16/16]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>0.402000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.340900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>0.302200</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=1504, training_loss=0.34835820320121785, metrics={'train_runtime': 515.1554, 'train_samples_per_second': 46.588, 'train_steps_per_second': 2.92, 'total_flos': 2052887697408000.0, 'train_loss': 0.34835820320121785, 'epoch': 16.0})"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "978923de-5909-4d90-a84c-d5cfe294d9de",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1500it [18:46,  1.33it/s]\n"
     ]
    }
   ],
   "source": [
    "test_pred_en_QA = predict(test_data_en, test_gold_en)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "d5f609e8-6595-4669-be82-f07a671345b3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "name  = 'test_pred_en_QA.txt'\n",
    "\n",
    "test_pred_en_df = []\n",
    "for i in test_pred_en_QA:\n",
    "    test_pred_en_df.append('\\t'.join(i))\n",
    "\n",
    "\n",
    "test_pred_en_df = pd.DataFrame(test_pred_en_df)\n",
    "test_pred_en_df.to_csv(name, header=None, index=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4e669ad4-dcfb-486e-8f98-a925885bc655",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MRR: 0.3711945535945533\n",
      "MAP: 0.24918376426043087\n",
      "P@1: 0.2826666666666667\n",
      "P@3: 0.21066666666666667\n",
      "P@5: 0.21613333333333243\n",
      "P@15: 0.29123732563732563\n"
     ]
    }
   ],
   "source": [
    "!python scorer.py /home/jovyan/work/SemEval2018-Task9/test/gold/1A.english.test.gold.txt test_pred_en_QA.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "756c71ec-8343-45bb-858a-a3e8e8c28632",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>MRR</th>\n",
       "      <th>MAP</th>\n",
       "      <th>P@1</th>\n",
       "      <th>P@3</th>\n",
       "      <th>P@5</th>\n",
       "      <th>P@15</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>prefix</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>what are hypernyms for hyponym [CHILD] ? [PARENTS]</th>\n",
       "      <td>0.37119</td>\n",
       "      <td>0.24918</td>\n",
       "      <td>0.28267</td>\n",
       "      <td>0.21067</td>\n",
       "      <td>0.21613</td>\n",
       "      <td>0.29124</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                        MRR      MAP      P@1  \\\n",
       "prefix                                                                          \n",
       "what are hypernyms for hyponym [CHILD] ? [PARENTS]  0.37119  0.24918  0.28267   \n",
       "\n",
       "                                                        P@3      P@5     P@15  \n",
       "prefix                                                                         \n",
       "what are hypernyms for hyponym [CHILD] ? [PARENTS]  0.21067  0.21613  0.29124  "
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f = io.StringIO()\n",
    "with redirect_stdout(f):\n",
    "    !python scorer.py /home/jovyan/work/SemEval2018-Task9/test/gold/1A.english.test.gold.txt test_pred_en_QA.txt\n",
    "\n",
    "\n",
    "_std_out = f.getvalue()\n",
    "_std_out = _std_out  + 'what are hypernyms for hyponym [CHILD] ? [PARENTS]'\n",
    "QA_table = answers(_std_out)\n",
    "QA_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "553d4415-d70d-4567-b734-bbfa53dca51d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "Fri Mar 31 17:19:28 2023       \n",
      "+-----------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 510.108.03   Driver Version: 510.108.03   CUDA Version: 11.6     |\n",
      "|-------------------------------+----------------------+----------------------+\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                               |                      |               MIG M. |\n",
      "|===============================+======================+======================|\n",
      "|   0  NVIDIA GeForce ...  Off  | 00000000:1B:00.0 Off |                  Off |\n",
      "| 30%   47C    P2    59W / 460W |   9537MiB / 24564MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   1  NVIDIA GeForce ...  Off  | 00000000:1C:00.0 Off |                  N/A |\n",
      "| 27%   34C    P8    15W / 260W |   9065MiB / 11264MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   2  NVIDIA GeForce ...  Off  | 00000000:1E:00.0 Off |                  N/A |\n",
      "| 27%   35C    P8    11W / 260W |   9743MiB / 11264MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   3  NVIDIA GeForce ...  Off  | 00000000:3F:00.0 Off |                  Off |\n",
      "|  0%   32C    P8    18W / 460W |      8MiB / 24564MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   4  NVIDIA GeForce ...  Off  | 00000000:40:00.0 Off |                  Off |\n",
      "|  0%   37C    P8    27W / 460W |   8082MiB / 24564MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   5  NVIDIA GeForce ...  Off  | 00000000:5E:00.0 Off |                  Off |\n",
      "|  0%   35C    P8    25W / 460W |      8MiB / 24564MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "                                                                               \n",
      "+-----------------------------------------------------------------------------+\n",
      "| Processes:                                                                  |\n",
      "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
      "|        ID   ID                                                   Usage      |\n",
      "|=============================================================================|\n",
      "+-----------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "cleanup()\n",
    "\n",
    "del trainer\n",
    "\n",
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b81b98eb-4601-4aa9-87e4-e41a93c51466",
   "metadata": {
    "tags": []
   },
   "source": [
    "# question: what are what are hypernyms for hyponym [hyponym] ? | answer: [hypernyms]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "d1bba4cf-caae-476f-9e2b-8eecb02d2e49",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    question: what are hypernyms for hyponym black...\n",
      "1    question: what are hypernyms for hyponym Turon...\n",
      "2    question: what are hypernyms for hyponym abhor...\n",
      "3    question: what are hypernyms for hyponym tropi...\n",
      "4    question: what are hypernyms for hyponym milit...\n",
      "Name: term, dtype: object\n",
      "0                           homopterous insect, insect\n",
      "1    technical specification, geologic timescale, p...\n",
      "2                      distaste, hatred, hate, disgust\n",
      "3    atmosphere, windstorm, violent storm, air curr...\n",
      "4                                       social control\n",
      "Name: hypernym, dtype: object\n",
      "0    question: what are hypernyms for hyponym malic...\n",
      "1    question: what are hypernyms for hyponym buckl...\n",
      "2    question: what are hypernyms for hyponym spelu...\n",
      "3    question: what are hypernyms for hyponym quo w...\n",
      "4    question: what are hypernyms for hyponym Jeff ...\n",
      "Name: term, dtype: object\n",
      "0       malevolence, distaste, hatred, hate, malignity\n",
      "1                                           body armor\n",
      "2                    exploration, adventurer, explorer\n",
      "3    proceedings, legal proceedings, proceeding, du...\n",
      "4               thrower, baseball player, jock, person\n",
      "Name: hypernym, dtype: object\n"
     ]
    }
   ],
   "source": [
    "def QandA_preprocessing(train_features, train_target, test_features, test_target):\n",
    "    \n",
    "    train_data_en = train_features.copy()\n",
    "    train_data_en = 'question: what are hypernyms for hyponym ' + train_data_en.term + ' ? | answer: '\n",
    "    print(train_data_en.head())\n",
    "\n",
    "    train_gold_en = train_target.copy()\n",
    "    train_gold_en = train_gold_en.hypernym.str.split('\\t').str.join(', ')\n",
    "    print(train_gold_en.head())\n",
    "    \n",
    "    test_data_en = test_features.copy()\n",
    "    test_data_en = 'question: what are hypernyms for hyponym '  + test_data_en.term + ' ? | answer: '\n",
    "    print(test_data_en.head())\n",
    "\n",
    "    test_gold_en = test_target.copy()\n",
    "    test_gold_en = test_gold_en.hypernym.str.split('\\t').str.join(', ')\n",
    "    print(test_gold_en.head())\n",
    "    \n",
    "    return train_data_en, train_gold_en, test_data_en, test_gold_en\n",
    "\n",
    "train_data_en, train_gold_en, test_data_en, test_gold_en = QandA_preprocessing(train_data_en_data, \n",
    "                                                                                 train_gold_en_data, \n",
    "                                                                                 test_data_en_data, \n",
    "                                                                                 test_gold_en_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "3a5d9583-5c05-412a-8e7a-252c26562da6",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = PairsDataset(tokenizer(train_data_en.tolist()), tokenizer(train_gold_en.tolist()))\n",
    "test_dataset = PairsDataset(tokenizer(test_data_en.tolist()), tokenizer(test_gold_en.tolist()))\n",
    "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n",
    "\n",
    "args = TrainingArguments(output_dir=\"t5-finetuned-large\", \n",
    "                         num_train_epochs=16, \n",
    "                         per_device_train_batch_size=16, save_steps=10000)\n",
    "\n",
    "trainer = Trainer(\n",
    "    model = model,\n",
    "    args = args,\n",
    "    train_dataset = train_dataset,\n",
    "    eval_dataset = test_dataset,\n",
    "    tokenizer = tokenizer,\n",
    "    data_collator = data_collator\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "df3bc8b8-4c0c-4162-b366-5cf4aafdc4b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/transformers/optimization.py:391: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "You're using a T5TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1504' max='1504' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1504/1504 08:38, Epoch 16/16]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>0.928500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.641400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>0.571500</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=1504, training_loss=0.7136728069249619, metrics={'train_runtime': 519.467, 'train_samples_per_second': 46.201, 'train_steps_per_second': 2.895, 'total_flos': 2560320657408000.0, 'train_loss': 0.7136728069249619, 'epoch': 16.0})"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "4d75ab8c-6f7c-40c6-931a-831a74334bbf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1500it [09:52,  2.53it/s]\n"
     ]
    }
   ],
   "source": [
    "test_pred_en_QA2 = predict(test_data_en, test_gold_en)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "16455035-fa33-4e67-9caf-48019f1de346",
   "metadata": {},
   "outputs": [],
   "source": [
    "name  = 'test_pred_en_QA2.txt'\n",
    "\n",
    "test_pred_en_df = []\n",
    "for i in test_pred_en_QA2:\n",
    "    test_pred_en_df.append('\\t'.join(i))\n",
    "\n",
    "\n",
    "test_pred_en_df = pd.DataFrame(test_pred_en_df)\n",
    "test_pred_en_df.to_csv(name, header=None, index=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "5183782a-840f-4a86-a53d-a1a6ed8c6808",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>MRR</th>\n",
       "      <th>MAP</th>\n",
       "      <th>P@1</th>\n",
       "      <th>P@3</th>\n",
       "      <th>P@5</th>\n",
       "      <th>P@15</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>prefix</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>question: what are hypernyms for hyponym [CHILD] ? | answer: [PARENTS]</th>\n",
       "      <td>0.4063</td>\n",
       "      <td>0.26081</td>\n",
       "      <td>0.32267</td>\n",
       "      <td>0.22644</td>\n",
       "      <td>0.23028</td>\n",
       "      <td>0.28664</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                       MRR      MAP      P@1  \\\n",
       "prefix                                                                         \n",
       "question: what are hypernyms for hyponym [CHILD...  0.4063  0.26081  0.32267   \n",
       "\n",
       "                                                        P@3      P@5     P@15  \n",
       "prefix                                                                         \n",
       "question: what are hypernyms for hyponym [CHILD...  0.22644  0.23028  0.28664  "
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f = io.StringIO()\n",
    "with redirect_stdout(f):\n",
    "    !python scorer.py /home/jovyan/work/SemEval2018-Task9/test/gold/1A.english.test.gold.txt test_pred_en_QA2.txt\n",
    "\n",
    "\n",
    "_std_out = f.getvalue()\n",
    "_std_out = _std_out  + 'question: what are hypernyms for hyponym [CHILD] ? | answer: [PARENTS]'\n",
    "QA2_table = answers(_std_out)\n",
    "QA2_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "fa41de05-1776-4428-b7b3-f85100d9a606",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "test_pred_en_df[0].str.split('\\t').apply(lambda x: x[:20]).str.join('\\t').to_csv('pruned.txt', header=None, index=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "9ed93246-9f0e-4d2c-a9ea-7e3382fea2ce",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "MRR: 0.40687333037333034\n",
      "MAP: 0.26098189970523306\n",
      "P@1: 0.3233333333333333\n",
      "P@3: 0.22666666666666643\n",
      "P@5: 0.2304111111111097\n",
      "P@15: 0.2867542346542343\n"
     ]
    }
   ],
   "source": [
    "!python scorer.py /home/jovyan/work/SemEval2018-Task9/test/gold/1A.english.test.gold.txt pruned.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "af0c040e-b5fe-4af6-9d1e-d2080f08a60d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>MRR</th>\n",
       "      <th>MAP</th>\n",
       "      <th>P@1</th>\n",
       "      <th>P@3</th>\n",
       "      <th>P@5</th>\n",
       "      <th>P@15</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>prefix</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>question: what are hypernyms for hyponym [CHILD] ? | answer: [PARENTS] + select top20</th>\n",
       "      <td>0.40687</td>\n",
       "      <td>0.26098</td>\n",
       "      <td>0.32333</td>\n",
       "      <td>0.22667</td>\n",
       "      <td>0.23041</td>\n",
       "      <td>0.28675</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                        MRR      MAP      P@1  \\\n",
       "prefix                                                                          \n",
       "question: what are hypernyms for hyponym [CHILD...  0.40687  0.26098  0.32333   \n",
       "\n",
       "                                                        P@3      P@5     P@15  \n",
       "prefix                                                                         \n",
       "question: what are hypernyms for hyponym [CHILD...  0.22667  0.23041  0.28675  "
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f = io.StringIO()\n",
    "with redirect_stdout(f):\n",
    "    !python scorer.py /home/jovyan/work/SemEval2018-Task9/test/gold/1A.english.test.gold.txt pruned.txt\n",
    "\n",
    "\n",
    "_std_out = f.getvalue()\n",
    "_std_out = _std_out  + 'question: what are hypernyms for hyponym [CHILD] ? | answer: [PARENTS] + select top20'\n",
    "QA2_pruned = answers(_std_out)\n",
    "QA2_pruned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "5a7a9d95-9244-44a5-ae2f-8fd47db41ffc",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "Fri Mar 31 17:57:34 2023       \n",
      "+-----------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 510.108.03   Driver Version: 510.108.03   CUDA Version: 11.6     |\n",
      "|-------------------------------+----------------------+----------------------+\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                               |                      |               MIG M. |\n",
      "|===============================+======================+======================|\n",
      "|   0  NVIDIA GeForce ...  Off  | 00000000:1B:00.0 Off |                  Off |\n",
      "| 30%   50C    P2    66W / 460W |   9681MiB / 24564MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   1  NVIDIA GeForce ...  Off  | 00000000:1C:00.0 Off |                  N/A |\n",
      "| 27%   34C    P8    14W / 260W |   9065MiB / 11264MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   2  NVIDIA GeForce ...  Off  | 00000000:1E:00.0 Off |                  N/A |\n",
      "| 27%   36C    P8    10W / 260W |      8MiB / 11264MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   3  NVIDIA GeForce ...  Off  | 00000000:3F:00.0 Off |                  Off |\n",
      "|  0%   32C    P8    18W / 460W |      8MiB / 24564MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   4  NVIDIA GeForce ...  Off  | 00000000:40:00.0 Off |                  Off |\n",
      "|  0%   37C    P8    28W / 460W |   8082MiB / 24564MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   5  NVIDIA GeForce ...  Off  | 00000000:5E:00.0 Off |                  Off |\n",
      "|  0%   36C    P8    26W / 460W |      8MiB / 24564MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "                                                                               \n",
      "+-----------------------------------------------------------------------------+\n",
      "| Processes:                                                                  |\n",
      "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
      "|        ID   ID                                                   Usage      |\n",
      "|=============================================================================|\n",
      "+-----------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "cleanup()\n",
    "\n",
    "del trainer\n",
    "\n",
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adc64973-408e-4ca7-9661-2aaf0033f2de",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Result "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "6f0015d3-45cb-42bc-b9d6-6fb06f81b07e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "c5a7a40d-7617-4526-a34b-06ba8cd3998b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style type=\"text/css\">\n",
       "#T_31b30_row0_col0, #T_31b30_row0_col3, #T_31b30_row2_col5, #T_31b30_row4_col1, #T_31b30_row4_col2, #T_31b30_row4_col4 {\n",
       "  color: white;\n",
       "  background-color: #1FC29D;\n",
       "}\n",
       "#T_31b30_row0_col5, #T_31b30_row2_col0, #T_31b30_row2_col1, #T_31b30_row2_col2, #T_31b30_row2_col3, #T_31b30_row2_col4 {\n",
       "  color: white;\n",
       "  background-color: #FF5555;\n",
       "}\n",
       "</style>\n",
       "<table id=\"T_31b30\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th class=\"blank level0\" >&nbsp;</th>\n",
       "      <th id=\"T_31b30_level0_col0\" class=\"col_heading level0 col0\" >MRR</th>\n",
       "      <th id=\"T_31b30_level0_col1\" class=\"col_heading level0 col1\" >MAP</th>\n",
       "      <th id=\"T_31b30_level0_col2\" class=\"col_heading level0 col2\" >P@1</th>\n",
       "      <th id=\"T_31b30_level0_col3\" class=\"col_heading level0 col3\" >P@3</th>\n",
       "      <th id=\"T_31b30_level0_col4\" class=\"col_heading level0 col4\" >P@5</th>\n",
       "      <th id=\"T_31b30_level0_col5\" class=\"col_heading level0 col5\" >P@15</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th class=\"index_name level0\" >prefix</th>\n",
       "      <th class=\"blank col0\" >&nbsp;</th>\n",
       "      <th class=\"blank col1\" >&nbsp;</th>\n",
       "      <th class=\"blank col2\" >&nbsp;</th>\n",
       "      <th class=\"blank col3\" >&nbsp;</th>\n",
       "      <th class=\"blank col4\" >&nbsp;</th>\n",
       "      <th class=\"blank col5\" >&nbsp;</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th id=\"T_31b30_level0_row0\" class=\"row_heading level0 row0\" >find hypernyms for hyponym: [CHILD] , target: [PARENTS]</th>\n",
       "      <td id=\"T_31b30_row0_col0\" class=\"data row0 col0\" >0.410240</td>\n",
       "      <td id=\"T_31b30_row0_col1\" class=\"data row0 col1\" >0.255700</td>\n",
       "      <td id=\"T_31b30_row0_col2\" class=\"data row0 col2\" >0.322670</td>\n",
       "      <td id=\"T_31b30_row0_col3\" class=\"data row0 col3\" >0.229560</td>\n",
       "      <td id=\"T_31b30_row0_col4\" class=\"data row0 col4\" >0.228990</td>\n",
       "      <td id=\"T_31b30_row0_col5\" class=\"data row0 col5\" >0.277180</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_31b30_level0_row1\" class=\"row_heading level0 row1\" >hyponym: [CHILD] | hypernyms: [PARENTS]</th>\n",
       "      <td id=\"T_31b30_row1_col0\" class=\"data row1 col0\" >0.394090</td>\n",
       "      <td id=\"T_31b30_row1_col1\" class=\"data row1 col1\" >0.250610</td>\n",
       "      <td id=\"T_31b30_row1_col2\" class=\"data row1 col2\" >0.308670</td>\n",
       "      <td id=\"T_31b30_row1_col3\" class=\"data row1 col3\" >0.220440</td>\n",
       "      <td id=\"T_31b30_row1_col4\" class=\"data row1 col4\" >0.216830</td>\n",
       "      <td id=\"T_31b30_row1_col5\" class=\"data row1 col5\" >0.282080</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_31b30_level0_row2\" class=\"row_heading level0 row2\" >what are hypernyms for hyponym [CHILD] ? [PARENTS]</th>\n",
       "      <td id=\"T_31b30_row2_col0\" class=\"data row2 col0\" >0.371190</td>\n",
       "      <td id=\"T_31b30_row2_col1\" class=\"data row2 col1\" >0.249180</td>\n",
       "      <td id=\"T_31b30_row2_col2\" class=\"data row2 col2\" >0.282670</td>\n",
       "      <td id=\"T_31b30_row2_col3\" class=\"data row2 col3\" >0.210670</td>\n",
       "      <td id=\"T_31b30_row2_col4\" class=\"data row2 col4\" >0.216130</td>\n",
       "      <td id=\"T_31b30_row2_col5\" class=\"data row2 col5\" >0.291240</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_31b30_level0_row3\" class=\"row_heading level0 row3\" >question: what are hypernyms for hyponym [CHILD] ? | answer: [PARENTS]</th>\n",
       "      <td id=\"T_31b30_row3_col0\" class=\"data row3 col0\" >0.406300</td>\n",
       "      <td id=\"T_31b30_row3_col1\" class=\"data row3 col1\" >0.260810</td>\n",
       "      <td id=\"T_31b30_row3_col2\" class=\"data row3 col2\" >0.322670</td>\n",
       "      <td id=\"T_31b30_row3_col3\" class=\"data row3 col3\" >0.226440</td>\n",
       "      <td id=\"T_31b30_row3_col4\" class=\"data row3 col4\" >0.230280</td>\n",
       "      <td id=\"T_31b30_row3_col5\" class=\"data row3 col5\" >0.286640</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_31b30_level0_row4\" class=\"row_heading level0 row4\" >question: what are hypernyms for hyponym [CHILD] ? | answer: [PARENTS] + select top20</th>\n",
       "      <td id=\"T_31b30_row4_col0\" class=\"data row4 col0\" >0.406870</td>\n",
       "      <td id=\"T_31b30_row4_col1\" class=\"data row4 col1\" >0.260980</td>\n",
       "      <td id=\"T_31b30_row4_col2\" class=\"data row4 col2\" >0.323330</td>\n",
       "      <td id=\"T_31b30_row4_col3\" class=\"data row4 col3\" >0.226670</td>\n",
       "      <td id=\"T_31b30_row4_col4\" class=\"data row4 col4\" >0.230410</td>\n",
       "      <td id=\"T_31b30_row4_col5\" class=\"data row4 col5\" >0.286750</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n"
      ],
      "text/plain": [
       "<pandas.io.formats.style.Styler at 0x7f8d38ea8d60>"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def highlight_max(s, props=''):\n",
    "    return np.where(s == np.nanmax(s.values), props, '')\n",
    "\n",
    "def highlight_min(s, props=''):\n",
    "    return np.where(s == np.nanmin(s.values), props, '')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "metrics_table = pd.concat([embading_find_hyponyms_uppercase_table, \n",
    "                           embading_standard_table,\n",
    "                           QA_table, \n",
    "                           QA2_table, \n",
    "                           QA2_pruned])\n",
    "\n",
    "\n",
    "metrics_table.to_csv('metrics_table.csv')\n",
    "metrics_table = metrics_table.style.apply(highlight_max, \n",
    "                                          props='color:white; background-color:#1FC29D', axis=0).apply(highlight_min, props='color:white; background-color:#FF5555', axis=0)\n",
    "metrics_table\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
