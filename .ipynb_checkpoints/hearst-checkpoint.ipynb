{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e4946738-b2f4-498b-b3e1-5232b71a9504",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mon Feb 27 15:24:49 2023       \n",
      "+-----------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 510.108.03   Driver Version: 510.108.03   CUDA Version: 11.6     |\n",
      "|-------------------------------+----------------------+----------------------+\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                               |                      |               MIG M. |\n",
      "|===============================+======================+======================|\n",
      "|   0  NVIDIA GeForce ...  Off  | 00000000:1B:00.0 Off |                  Off |\n",
      "|  0%   35C    P8    25W / 460W |  20619MiB / 24564MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   1  NVIDIA GeForce ...  Off  | 00000000:1C:00.0 Off |                  N/A |\n",
      "| 52%   51C    P8    18W / 260W |   7823MiB / 11264MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   2  NVIDIA GeForce ...  Off  | 00000000:1E:00.0 Off |                  N/A |\n",
      "| 32%   43C    P8    11W / 260W |      8MiB / 11264MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   3  NVIDIA GeForce ...  Off  | 00000000:3F:00.0 Off |                  Off |\n",
      "| 30%   57C    P2   229W / 460W |   5153MiB / 24564MiB |     42%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   4  NVIDIA GeForce ...  Off  | 00000000:40:00.0 Off |                  Off |\n",
      "| 54%   70C    P2   255W / 460W |  11431MiB / 24564MiB |     14%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   5  NVIDIA GeForce ...  Off  | 00000000:5E:00.0 Off |                  Off |\n",
      "| 32%   54C    P8    30W / 460W |      8MiB / 24564MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "                                                                               \n",
      "+-----------------------------------------------------------------------------+\n",
      "| Processes:                                                                  |\n",
      "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
      "|        ID   ID                                                   Usage      |\n",
      "|=============================================================================|\n",
      "+-----------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "e167a3bc-5f8b-4e15-8c61-d427dfdcac4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install torch\n",
    "!pip install transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "8c6b35a9-023c-4ff8-8739-bc978c1a4be0",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/jovyan/nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /home/jovyan/nltk_data...\n",
      "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "from transformers import T5ForConditionalGeneration, AutoTokenizer\n",
    "from transformers import AutoModelForSeq2SeqLM, AutoTokenizer, AutoConfig\n",
    "\n",
    "\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import Trainer, TrainingArguments\n",
    "from transformers.file_utils import cached_property\n",
    "from typing import Tuple\n",
    "from sklearn.model_selection import train_test_split\n",
    "import gc\n",
    "from tqdm.auto import tqdm, trange\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "from sklearn.utils import shuffle\n",
    "\n",
    "from collections import Counter\n",
    "from tqdm import tqdm\n",
    "\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9a913f3-d530-4177-b1a6-97a87397db80",
   "metadata": {},
   "source": [
    "# model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d3f49cbf-db6e-4ce6-addd-d1301fd93a57",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mon Feb 27 15:27:10 2023       \n",
      "+-----------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 510.108.03   Driver Version: 510.108.03   CUDA Version: 11.6     |\n",
      "|-------------------------------+----------------------+----------------------+\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                               |                      |               MIG M. |\n",
      "|===============================+======================+======================|\n",
      "|   0  NVIDIA GeForce ...  Off  | 00000000:1B:00.0 Off |                  Off |\n",
      "|  0%   35C    P8    25W / 460W |  20619MiB / 24564MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   1  NVIDIA GeForce ...  Off  | 00000000:1C:00.0 Off |                  N/A |\n",
      "| 45%   48C    P8    17W / 260W |   7823MiB / 11264MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   2  NVIDIA GeForce ...  Off  | 00000000:1E:00.0 Off |                  N/A |\n",
      "| 30%   41C    P8    12W / 260W |      8MiB / 11264MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   3  NVIDIA GeForce ...  Off  | 00000000:3F:00.0 Off |                  Off |\n",
      "| 30%   55C    P2   208W / 460W |   5153MiB / 24564MiB |     37%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   4  NVIDIA GeForce ...  Off  | 00000000:40:00.0 Off |                  Off |\n",
      "| 55%   78C    P2   391W / 460W |  11431MiB / 24564MiB |     70%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   5  NVIDIA GeForce ...  Off  | 00000000:5E:00.0 Off |                  Off |\n",
      "| 31%   53C    P8    30W / 460W |      8MiB / 24564MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "                                                                               \n",
      "+-----------------------------------------------------------------------------+\n",
      "| Processes:                                                                  |\n",
      "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
      "|        ID   ID                                                   Usage      |\n",
      "|=============================================================================|\n",
      "+-----------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ba76c0c3-a3c0-40ea-b5f4-1b7a8ad9ac8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]='3'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8a26c278-e3ee-4084-97f6-dac02ab584b9",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU\n"
     ]
    }
   ],
   "source": [
    "if torch.cuda.is_available():\n",
    "    device='cuda'\n",
    "    print('GPU')\n",
    "else:\n",
    "    device='cpu'\n",
    "    print('CPU')\n",
    "    \n",
    "    \n",
    "SEED = 0\n",
    "torch.manual_seed(SEED)\n",
    "torch.cuda.manual_seed(SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ec73cecd-abb8-4635-a184-430959e38258",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "51bb2c4748e54f7cbfae665d50f2fc08",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)lve/main/config.json:   0%|          | 0.00/1.21k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8ddfda838fd5454a83a5846565d0119b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)\"pytorch_model.bin\";:   0%|          | 0.00/2.95G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cd4cec6e049844bebdcac9cb761badf0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)neration_config.json:   0%|          | 0.00/147 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dcdd3bdf9f9b466cbc30cf7ea16aa5e2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)ve/main/spiece.model:   0%|          | 0.00/792k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e4cfde97ab3f42b88109cde74a24909a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)/main/tokenizer.json:   0%|          | 0.00/1.39M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/transformers/models/t5/tokenization_t5_fast.py:155: FutureWarning: This tokenizer was incorrectly instantiated with a model max length of 512 which will be corrected in Transformers v5.\n",
      "For now, this behavior is kept to avoid breaking backwards compatibility when padding/encoding with `truncation is True`.\n",
      "- Be aware that you SHOULD NOT rely on t5-large automatically truncating your input to 512 when padding/encoding.\n",
      "- If you want to encode/pad to sequences longer than 512 you can either instantiate this tokenizer with `model_max_length` or pass `max_length` when encoding/padding.\n",
      "- To avoid this warning, please instantiate this tokenizer with `model_max_length` set to your preferred value.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from transformers import TrainingArguments, Trainer\n",
    "\n",
    "model_checkpoint = \"t5-large\"\n",
    "model = T5ForConditionalGeneration.from_pretrained(model_checkpoint).to(device)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3171f4cc-6472-4797-883e-2cb95c4d0c52",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class PairsDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, x, y):\n",
    "        self.x = x\n",
    "        self.y = y\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        assert idx <= len(self.x['input_ids']), (idx, len(self.x['input_ids']))\n",
    "        item = {key: val[idx] for key, val in self.x.items()}\n",
    "        item['decoder_attention_mask'] = self.y['attention_mask'][idx]\n",
    "        item['labels'] = self.y['input_ids'][idx]\n",
    "        return item\n",
    "    \n",
    "    @property\n",
    "    def n(self):\n",
    "        return len(self.x['input_ids'])\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "87b3137b-4300-4c74-9822-6aaefce55e37",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from typing import List, Dict, Union\n",
    "\n",
    "class DataCollatorWithPadding:\n",
    "    def __init__(self, tokenizer):\n",
    "        self.tokenizer = tokenizer\n",
    "\n",
    "    def __call__(self, features: List[Dict[str, Union[List[int], torch.Tensor]]]) -> Dict[str, torch.Tensor]:\n",
    "        batch = self.tokenizer.pad(\n",
    "            features,\n",
    "            padding=True,\n",
    "        )\n",
    "        ybatch = self.tokenizer.pad(\n",
    "            {'input_ids': batch['labels'], 'attention_mask': batch['decoder_attention_mask']},\n",
    "            padding=True,\n",
    "        ) \n",
    "        batch['labels'] = ybatch['input_ids']\n",
    "        batch['decoder_attention_mask'] = ybatch['attention_mask']\n",
    "        \n",
    "        return {k: torch.tensor(v) for k, v in batch.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7ef1229c-d458-413c-acc9-2fb0221248b9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def cleanup():\n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()\n",
    "    \n",
    "cleanup()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ce916472-8e8c-4436-8028-9fc1ec6ac61a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def evaluate_model(model, test_dataloader):\n",
    "    num = 0\n",
    "    den = 0\n",
    "\n",
    "    for batch in test_dataloader:\n",
    "        with torch.no_grad():\n",
    "            loss = model(**{k: v.to(model.device) for k, v in batch.items()}).loss\n",
    "            num += len(batch) * loss.item()\n",
    "            den += len(batch)\n",
    "    val_loss = num / den\n",
    "    return val_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6c063e4-fa59-487c-96a0-d9a8cf7f35f5",
   "metadata": {},
   "source": [
    "# **Read data SemEval2018-Task9**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c2344944-6187-49b1-bcb9-0f1b98c505ce",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>term</th>\n",
       "      <th>relation</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>blackfly</td>\n",
       "      <td>Concept</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Turonian</td>\n",
       "      <td>Entity</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>abhorrence</td>\n",
       "      <td>Concept</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>tropical storm</td>\n",
       "      <td>Concept</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>militarization</td>\n",
       "      <td>Concept</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             term relation\n",
       "0        blackfly  Concept\n",
       "1        Turonian   Entity\n",
       "2      abhorrence  Concept\n",
       "3  tropical storm  Concept\n",
       "4  militarization  Concept"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "path_data_en = \"SemEval2018-Task9/training/data/1A.english.training.data.txt\"\n",
    "path_gold_en = \"SemEval2018-Task9/training/gold/1A.english.training.gold.txt\"\n",
    "\n",
    "train_data_en_data = pd.read_csv(path_data_en, header=None, sep=\"\\t\", names=['term', 'relation'])\n",
    "train_gold_en_data = pd.read_csv(path_gold_en, header=None, names=['hypernym'])\n",
    "\n",
    "train_data_en_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "129edda7-bf42-43e4-9fd0-7e55aa8bc714",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "path_test_data_en = \"SemEval2018-Task9/test/data/1A.english.test.data.txt\"\n",
    "path_test_gold_en = \"SemEval2018-Task9/test/gold/1A.english.test.gold.txt\"\n",
    "\n",
    "test_data_en_data = pd.read_csv(path_test_data_en, header=None, sep=\"\\t\", names=['term', 'relation'])\n",
    "test_gold_en_data = pd.read_csv(path_test_gold_en, header=None, names=['hypernym'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "37744552-c78d-4308-8c36-a256835839ea",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['There are a lot of [PARENT] such as',\n",
       " 'There were a lot of [PARENT] such as',\n",
       " 'There are a lot of [PARENT] here such as',\n",
       " 'Other [PARENT] such as',\n",
       " 'My favorite [PARENT] is either',\n",
       " 'There were a lot of [PARENT] here such as',\n",
       " 'which includes various [PARENT] like',\n",
       " 'Other [PARENT] especially',\n",
       " 'which includes various [PARENT] such as',\n",
       " 'My favorite [PARENT] is',\n",
       " 'I know such types of [PARENT] as',\n",
       " 'I know such kinds of [PARENT] as',\n",
       " '[PARENT] such as',\n",
       " 'I know many kinds of [PARENT] for example',\n",
       " 'Other [PARENT] for example',\n",
       " '[PARENT] ndmely',\n",
       " 'I know many types of [PARENT] for example',\n",
       " '[PARENT] including',\n",
       " 'There are a lot of [PARENT] for example',\n",
       " 'which includes various [PARENT] for example',\n",
       " 'There are a lot of [PARENT] here for example',\n",
       " '[PARENT] e.g.',\n",
       " '[PARENT] like',\n",
       " '[PARENT] especially',\n",
       " '[PARENT] for example',\n",
       " '[PARENT] for instance']"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hearst_patterns = \"\"\"\n",
    "There are a lot of [PARENT] such as\n",
    "\n",
    "There were a lot of [PARENT] such as\n",
    "\n",
    "There are a lot of [PARENT] here such as\n",
    "\n",
    "Other [PARENT] such as\n",
    "\n",
    "My favorite [PARENT] is either\n",
    "\n",
    "There were a lot of [PARENT] here such as\n",
    "\n",
    "which includes various [PARENT] like\n",
    "\n",
    "Other [PARENT] especially\n",
    "\n",
    "which includes various [PARENT] such as\n",
    "\n",
    "My favorite [PARENT] is\n",
    "\n",
    "I know such types of [PARENT] as\n",
    "\n",
    "I know such kinds of [PARENT] as\n",
    "\n",
    "[PARENT] such as\n",
    "\n",
    "I know many kinds of [PARENT] for example\n",
    "\n",
    "Other [PARENT] for example\n",
    "\n",
    "[PARENT] ndmely\n",
    "\n",
    "I know many types of [PARENT] for example\n",
    "\n",
    "[PARENT] including\n",
    "\n",
    "There are a lot of [PARENT] for example\n",
    "\n",
    "which includes various [PARENT] for example\n",
    "\n",
    "There are a lot of [PARENT] here for example\n",
    "\n",
    "[PARENT] e.g.\n",
    "\n",
    "[PARENT] like\n",
    "\n",
    "[PARENT] especially\n",
    "\n",
    "[PARENT] for example\n",
    "\n",
    "[PARENT] for instance\n",
    "\"\"\"\n",
    "\n",
    "hearst_patterns = [pattern for ind, pattern in enumerate(hearst_patterns.split('\\n')) if ind % 2 != 0]\n",
    "hearst_patterns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "c626a99f-dcc7-44d3-b1cc-1150671d9133",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def hearest_preprocessing(train_features, train_target, test_features, test_target, hearst_pattern):\n",
    "    \n",
    "    stage_1, stage_2 = hearst_patterns[0].split('[PARENT]')\n",
    "    \n",
    "    train_data_en = train_features.copy()\n",
    "    train_data_en = stage_1 + train_data_en.term + stage_2\n",
    "    print(train_data_en.head())\n",
    "\n",
    "    train_gold_en = train_target.copy()\n",
    "    train_gold_en = train_gold_en.hypernym.str.split('\\t').str.join(', ')\n",
    "    print(train_gold_en.head())\n",
    "    \n",
    "    test_data_en = test_features.copy()\n",
    "    test_data_en = stage_1 + test_data_en.term + stage_2\n",
    "    print(test_data_en.head())\n",
    "\n",
    "    test_gold_en = test_target.copy()\n",
    "    test_gold_en = test_gold_en.hypernym.str.split('\\t').str.join(', ')\n",
    "    print(test_gold_en.head())\n",
    "    \n",
    "    return train_data_en, train_gold_en, test_data_en, test_gold_en\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "104ea460-4fcd-44c6-85f6-44af748bc722",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0          There are a lot of blackfly such as\n",
      "1          There are a lot of Turonian such as\n",
      "2        There are a lot of abhorrence such as\n",
      "3    There are a lot of tropical storm such as\n",
      "4    There are a lot of militarization such as\n",
      "Name: term, dtype: object\n",
      "0                           homopterous insect, insect\n",
      "1    technical specification, geologic timescale, p...\n",
      "2                      distaste, hatred, hate, disgust\n",
      "3    atmosphere, windstorm, violent storm, air curr...\n",
      "4                                       social control\n",
      "Name: hypernym, dtype: object\n",
      "0    There are a lot of maliciousness such as\n",
      "1          There are a lot of buckler such as\n",
      "2        There are a lot of spelunker such as\n",
      "3     There are a lot of quo warranto such as\n",
      "4     There are a lot of Jeff Francis such as\n",
      "Name: term, dtype: object\n",
      "0       malevolence, distaste, hatred, hate, malignity\n",
      "1                                           body armor\n",
      "2                    exploration, adventurer, explorer\n",
      "3    proceedings, legal proceedings, proceeding, du...\n",
      "4               thrower, baseball player, jock, person\n",
      "Name: hypernym, dtype: object\n"
     ]
    }
   ],
   "source": [
    "train_data_en, train_gold_en, test_data_en, test_gold_en = hearest_preprocessing(train_data_en_data, \n",
    "                                                                                 train_gold_en_data, \n",
    "                                                                                 test_data_en_data, \n",
    "                                                                                 test_gold_en_data, \n",
    "                                                                                 hearst_patterns[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b52fad45-8557-4569-85e9-84a9c8de4d1c",
   "metadata": {},
   "source": [
    "# Model training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "18b98665-8190-48db-9128-eaf597e80615",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/transformers/models/t5/tokenization_t5_fast.py:155: FutureWarning: This tokenizer was incorrectly instantiated with a model max length of 512 which will be corrected in Transformers v5.\n",
      "For now, this behavior is kept to avoid breaking backwards compatibility when padding/encoding with `truncation is True`.\n",
      "- Be aware that you SHOULD NOT rely on t5-large automatically truncating your input to 512 when padding/encoding.\n",
      "- If you want to encode/pad to sequences longer than 512 you can either instantiate this tokenizer with `model_max_length` or pass `max_length` when encoding/padding.\n",
      "- To avoid this warning, please instantiate this tokenizer with `model_max_length` set to your preferred value.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# use only 1 time per session\n",
    "model_checkpoint = \"t5-large\"\n",
    "model = T5ForConditionalGeneration.from_pretrained(model_checkpoint).to(device)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_checkpoint, max_length=100, block_size=64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f9a20d05-cc19-4151-88b2-e044f4650bc2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# train_dataset = PairsDataset(tokenizer(train_data_en.tolist()), tokenizer(train_gold_en.tolist()))\n",
    "# test_dataset = PairsDataset(tokenizer(test_data_en.tolist()), tokenizer(test_gold_en.tolist()))\n",
    "# data_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n",
    "\n",
    "# args = TrainingArguments(output_dir=\"t5-finetuned-large\", \n",
    "#                          num_train_epochs=16, \n",
    "#                          per_device_train_batch_size=16, save_steps=10000)\n",
    "\n",
    "# trainer = Trainer(\n",
    "#     model = model,\n",
    "#     args = args,\n",
    "#     train_dataset = train_dataset,\n",
    "#     eval_dataset = test_dataset,\n",
    "#     tokenizer = tokenizer,\n",
    "#     data_collator = data_collator\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "d1d24206-3655-468e-b961-964bde0bc939",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mon Feb 27 16:41:57 2023       \n",
      "+-----------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 510.108.03   Driver Version: 510.108.03   CUDA Version: 11.6     |\n",
      "|-------------------------------+----------------------+----------------------+\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                               |                      |               MIG M. |\n",
      "|===============================+======================+======================|\n",
      "|   0  NVIDIA GeForce ...  Off  | 00000000:1B:00.0 Off |                  Off |\n",
      "|  0%   35C    P8    25W / 460W |  20619MiB / 24564MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   1  NVIDIA GeForce ...  Off  | 00000000:1C:00.0 Off |                  N/A |\n",
      "| 45%   55C    P8    18W / 260W |   7823MiB / 11264MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   2  NVIDIA GeForce ...  Off  | 00000000:1E:00.0 Off |                  N/A |\n",
      "| 27%   41C    P8    11W / 260W |      8MiB / 11264MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   3  NVIDIA GeForce ...  Off  | 00000000:3F:00.0 Off |                  Off |\n",
      "| 33%   59C    P2   231W / 460W |   5153MiB / 24564MiB |      9%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   4  NVIDIA GeForce ...  Off  | 00000000:40:00.0 Off |                  Off |\n",
      "| 56%   68C    P2   182W / 460W |  11431MiB / 24564MiB |     10%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   5  NVIDIA GeForce ...  Off  | 00000000:5E:00.0 Off |                  Off |\n",
      "| 73%   88C    P2   402W / 460W |  12718MiB / 24564MiB |     86%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "                                                                               \n",
      "+-----------------------------------------------------------------------------+\n",
      "| Processes:                                                                  |\n",
      "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
      "|        ID   ID                                                   Usage      |\n",
      "|=============================================================================|\n",
      "+-----------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eaec1fb2-d985-4533-9bf0-e6eb06f31f4f",
   "metadata": {
    "tags": []
   },
   "source": [
    "# EVALUATION "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "088167d0-d65c-4b0c-aeea-3345f8db8412",
   "metadata": {},
   "outputs": [],
   "source": [
    "example = test_data_en.tolist()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "38d41a81-f8af-460d-80f8-7c1908e2870f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['type of phishing that can be found in the internet nowadays. as cybercrime',\n",
       " 'malware and so on which can harm your computer or phone. as a lot of spamming which are very harmful. as spam',\n",
       " 'virus',\n",
       " 'a lot of maliciousness. as phishing. as malware. as virus',\n",
       " 'malware',\n",
       " 'as hacking and a lot of maliciousness. as virus',\n",
       " 'and so on. as spam. as as scam.ness such as malicious. as hacking... as.. There are so many malicious... Then there are malicious. Such as........... as viruses',\n",
       " 'worms',\n",
       " 'trojan horses',\n",
       " 'etc.',\n",
       " 'trojans etc. It',\n",
       " 'trojans etc. ',\n",
       " 'trojans etc. In',\n",
       " 'and so on. as spam. as as scam.ness such as malicious. as hacking... as.. There are so many malicious... Then there are malicious. Such as........... as bots',\n",
       " 'viruses',\n",
       " 'trojans etc. It is',\n",
       " 'trojans etc.,',\n",
       " 'trojans etc. These',\n",
       " 'trojans etc. which',\n",
       " 'trojans etc. They',\n",
       " 'trojan',\n",
       " 'etc. It',\n",
       " 'trojan etc. as phishing. as malware. as cybercrime. as as spam.ness such as as scam... as.. There are so many malignantness like as as..... Then there are some of the malicious. malware and as malicious as viruses. as as fraud. as as the as',\n",
       " 'trojan etc. as phishing. as malware. as cybercrime. as as spam.ness such as as scam... as.. There are so many malignantness like as as..... Then there are some of the malicious. malware and as malicious as viruses. as as as fraud. as the as',\n",
       " 'trojan etc. as phishing. as malware. as cybercrime. as as spam.ness such as as scam... as.. There are so many malignantness like as as..... Then there are some of the malicious. malware and as malicious as viruses. as the as as as as bots',\n",
       " 'trojan etc. as phishing. as malware. as cybercrime. as as spam.ness such as as scam... as.. There are so many malignantness like as as..... Then there are some of the malicious. malware and as malicious as viruses. as as fraud. as as as the',\n",
       " 'trojan etc. as phishing. as malware. as cybercrime. as as spam.ness such as as scam... as.. There are so many malignantness like as as..... Then there are some of the malicious. malware and as malicious as viruses. as the as as as as botnet',\n",
       " 'trojan etc. as phishing. as malware. as cybercrime. as as spam.ness such as as scam... as.. There are so many malignantness like as as..... Then there are some of the malicious. malware and as malicious as viruses. as as as fraud. as as the',\n",
       " 'trojan etc. as phishing. as malware. as cybercrime. as as spam.ness such as as scam... as.. There are so many malignantness like as as..... Then there are some of the malicious. malware and as malicious as viruses. as as fraud. as as as bot',\n",
       " 'trojan etc. as phishing. as malware. as cybercrime. as as spam.ness such as as scam... as.. There are so many malignantness like as as..... Then there are some of the malicious. malware and as malicious as viruses. as as as fraud. as as bot',\n",
       " 'trojan etc. as phishing. as malware. as cybercrime. as as spam.ness such as as scam... as.. There are so many malignantness like as as..... Then there are some of the malicious. malware and as malicious as viruses. as as fraud. as as as tro',\n",
       " 'trojan etc. as phishing. as malware. as cybercrime. as as spam.ness such as as scam... as.. There are so many malignantness like as as..... Then there are some of the malicious. malware and as malicious as viruses. as as as fraud. as as tro',\n",
       " 'etc in cyberspace. as scam. as as hacking,ness. as as as fake as as like such as maliciousness malicious malicious malicious like the such like.a',\n",
       " 'etc in cyberspace. as scam. as as hacking,ness. as as as fake as as like such as maliciousness malicious malicious malicious like the such malware.a',\n",
       " 'etc in cyberspace. as scam. as as hacking,ness. as as as fake as as like such as maliciousness malicious malicious malicious like the such likeaa',\n",
       " 'etc in cyberspace. as scam. as as hacking,ness. as as as fake as as like such as maliciousness malicious malicious malicious like the such malwareaa',\n",
       " 'etc in cyberspace. as scam. as as hacking,ness. as as as fake as as like such as maliciousness malicious malicious malicious like the such like.e',\n",
       " 'etc in cyberspace. as scam. as as hacking,ness. as as as fake as as like such as maliciousness malicious malicious malicious like the such cyberaa',\n",
       " 'etc in cyberspace. as scam. as as hacking,ness. as as as fake as as like such as maliciousness malicious malicious malicious like the such malware.e',\n",
       " 'etc in cyberspace. as scam. as as hacking,ness. as as as fake as as like such as maliciousness malicious malicious malicious like the such the.a',\n",
       " 'etc in cyberspace. as scam. as as hacking,ness. as as as fake as as like such as maliciousness malicious malicious malicious like the such theaa',\n",
       " 'etc in cyberspace. as scam. as as hacking,ness. as as as fake as as like such as maliciousness malicious malicious malicious like malware. such suchaa',\n",
       " 'etc in cyberspace. as scam. as as hacking,ness. as as as fake as as like such as maliciousness malicious malicious malicious like the such like.',\n",
       " 'etc in cyberspace. as scam. as as hacking,ness. as as as fake as as like such as maliciousness malicious malicious malicious like the such malware.',\n",
       " 'etc in cyberspace. as scam. as as hacking,ness. as as as fake as as like such as maliciousness malicious malicious malicious like malware. such sucha',\n",
       " 'etc in cyberspace. as scam. as as hacking,ness. as as as fake as as like such as maliciousness malicious malicious malicious like the such likea',\n",
       " 'etc in cyberspace. as scam. as as hacking,ness. as as as fake as as like such as maliciousness malicious malicious malicious like the such the.',\n",
       " 'etc in cyberspace. as scam. as as hacking,ness. as as as fake as as like such as maliciousness malicious malicious malicious like malware. such such',\n",
       " 'etc in cyberspace. as scam. as as hacking,ness. as as as fake as as like such as maliciousness malicious malicious malicious like the such scam.',\n",
       " 'etc in cyberspace. as scam. as as hacking,ness. as as as fake as as like such as maliciousness malicious malicious malicious like the such malwarea',\n",
       " 'etc in cyberspace. as scam. as as hacking,ness. as as as fake as as like such as maliciousness malicious malicious malicious like the such thea',\n",
       " 'etc in cyberspace. as scam. as as hacking,ness. as as as fake as as like such as maliciousness malicious malicious malicious like the such cybera',\n",
       " 'etc in cyberspace. as scam. as as hacking,ness. as as as fake as as like such as maliciousness malicious malicious malicious like the such malware',\n",
       " 'etc in cyberspace. as scam. as as hacking,ness. as as as fake as as like such as maliciousness malicious malicious malicious like the such scam',\n",
       " 'etc in cyberspace. as scam. as as hacking,ness. as as as fake as as like such as maliciousness malicious malicious malicious like the such like',\n",
       " 'etc in cyberspace. as scam. as as hacking,ness. as as as fake as as like such as maliciousness malicious malicious malicious like malware. scama',\n",
       " 'etc in cyberspace. as scam. as as hacking,ness. as as as fake as as like such as maliciousness malicious malicious malicious like the such spam',\n",
       " 'etc in cyberspace. as scam. as as hacking,ness. as as as fake as as like such as maliciousness malicious malicious malicious like the such cyber',\n",
       " 'etc in cyberspace. as scam. as as hacking,ness. as as as fake as as like such as maliciousness malicious malicious malicious like the such the',\n",
       " 'etc in cyberspace. as scam. as as hacking,ness. as as as fake as as like such as maliciousness malicious malicious malicious like the malware malware',\n",
       " 'etc in cyberspace. as scam. as as hacking,ness. as as as fake as as like such as maliciousness malicious malicious malicious like like malware malware',\n",
       " 'etc in cyberspace. as scam. as as hacking,ness. as as as fake as as like such as maliciousness malicious malicious malicious like malware. scam']"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_ids = tokenizer.encode(example, return_tensors=\"pt\")\n",
    "output_batch = model.generate(input_ids.cuda(), no_repeat_ngram_size=2, max_new_tokens=2048, \n",
    "                              num_return_sequences=50, num_beams=50, early_stopping=True, \n",
    "                              num_beam_groups=5, \n",
    "                              diversity_penalty=1.0)\n",
    "decoded_list = []\n",
    "for outputs in output_batch:\n",
    "    decoded = tokenizer.decode(outputs, skip_special_tokens=True).split(\", \")\n",
    "    decoded_list.extend(decoded)\n",
    "    \n",
    "sorted_predicted_answer = [i[0] for i in Counter(decoded_list).most_common()]\n",
    "sorted_predicted_answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "94edd7e6-385b-4747-99a1-b98bba45590b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1500/1500 [56:21<00:00,  2.25s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 56min 14s, sys: 9.19 s, total: 56min 23s\n",
      "Wall time: 56min 21s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "%%time \n",
    "test_pred_en=[]\n",
    "for i_2 in tqdm(test_data_en.tolist()):\n",
    "        input_ids = tokenizer.encode(i_2, return_tensors=\"pt\")\n",
    "        output_batch = model.generate(input_ids.cuda(), no_repeat_ngram_size=2, max_new_tokens=2048, \n",
    "                                      num_return_sequences=50, num_beams=50, early_stopping=True, \n",
    "                                      num_beam_groups=5, \n",
    "                                      diversity_penalty=1.0)\n",
    "        decoded_list = []\n",
    "        for outputs in output_batch:\n",
    "            decoded = tokenizer.decode(outputs, skip_special_tokens=True).split(\", \")\n",
    "            decoded_list.extend(decoded)\n",
    "\n",
    "        sorted_predicted_answer = [i[0] for i in Counter(decoded_list).most_common()]\n",
    "        \n",
    "        test_pred_en.append(sorted_predicted_answer)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "c019a190-e23a-431d-86d6-e6e0f12cf915",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "name  = 'test_pred_hearst_pattern.txt'\n",
    "\n",
    "test_pred_en_df = []\n",
    "for i in test_pred_en:\n",
    "    test_pred_en_df.append('\\t'.join(i))\n",
    "\n",
    "\n",
    "test_pred_en_df = pd.DataFrame(test_pred_en_df)\n",
    "test_pred_en_df.to_csv(name, header=None, index=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "1ac2940d-da6d-4a5f-b173-c292c190bc70",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "MRR: 0.0025351851851851855\n",
      "MAP: 0.002115307531974199\n",
      "P@1: 0.0\n",
      "P@3: 0.001888888888888889\n",
      "P@5: 0.002333333333333334\n",
      "P@15: 0.0024685185185185188\n"
     ]
    }
   ],
   "source": [
    "!python debuged_task9-scorer.py SemEval2018-Task9/test/gold/1A.english.test.gold.txt test_pred_hearst_pattern.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "a0d0537b-b9c1-452d-a98d-c24e46a69228",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    }
   ],
   "source": [
    "from contextlib import redirect_stdout\n",
    "import io\n",
    "\n",
    "f = io.StringIO()\n",
    "with redirect_stdout(f):\n",
    "    !python debuged_task9-scorer.py SemEval2018-Task9/test/gold/1A.english.test.gold.txt test_pred_hearst_pattern.txt\n",
    "_std_out = f.getvalue()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "03be1639-0415-4dd9-825f-7b3e800b7117",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'MRR: 0.0025351851851851855\\r\\nMAP: 0.002115307531974199\\r\\nP@1: 0.0\\r\\nP@3: 0.001888888888888889\\r\\nP@5: 0.002333333333333334\\r\\nP@15: 0.0024685185185185188\\r\\nname_of_pattern'"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# _std_out = _std_out.replace('\\r', '')\n",
    "_std_out = _std_out  + 'name_of_pattern'\n",
    "_std_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "22db2593-ddfb-4205-8405-f245d7a0eab4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['MRR: 0.0025351851851851855',\n",
       " 'MAP: 0.002115307531974199',\n",
       " 'P@1: 0.0',\n",
       " 'P@3: 0.001888888888888889',\n",
       " 'P@5: 0.002333333333333334',\n",
       " 'P@15: 0.0024685185185185188',\n",
       " 'name_of_pattern']"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_std_out.split('\\r\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "1396bcf7-12a2-4e4d-b520-1f26edbbcbd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def answers(str_ans, name_of_pattern):\n",
    "    str_ans += name_of_pattern\n",
    "    columns_name = []\n",
    "    values = []\n",
    "    for ind, metrics in enumerate(_std_out.split('\\r\\n')):\n",
    "        print(metrics)\n",
    "        if ind == 6:\n",
    "            _name = 'pattern'\n",
    "            number = metrics\n",
    "        else:\n",
    "            _name, number = metrics.split(' ')\n",
    "            number = round(float(number), 5)\n",
    "            _name = _name[:-1]\n",
    "        \n",
    "        columns_name.append(_name)\n",
    "        values.append([number])\n",
    "        \n",
    "    \n",
    "        \n",
    "    df = pd.DataFrame(values).T\n",
    "    df.columns = columns_name\n",
    "    df.set_index('pattern', inplace=True)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "9f50f849-4cb6-4bef-afb4-9cc6fa54ea8f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MRR: 0.0025351851851851855\n",
      "MAP: 0.002115307531974199\n",
      "P@1: 0.0\n",
      "P@3: 0.001888888888888889\n",
      "P@5: 0.002333333333333334\n",
      "P@15: 0.0024685185185185188\n",
      "name_of_pattern\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>MRR</th>\n",
       "      <th>MAP</th>\n",
       "      <th>P@1</th>\n",
       "      <th>P@3</th>\n",
       "      <th>P@5</th>\n",
       "      <th>P@15</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>pattern</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>name_of_pattern</th>\n",
       "      <td>0.00254</td>\n",
       "      <td>0.00212</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00189</td>\n",
       "      <td>0.00233</td>\n",
       "      <td>0.00247</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                     MRR      MAP  P@1      P@3      P@5     P@15\n",
       "pattern                                                          \n",
       "name_of_pattern  0.00254  0.00212  0.0  0.00189  0.00233  0.00247"
      ]
     },
     "execution_count": 138,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "answers(ans, 'name_of_pattern')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "faf5dac5-4e9e-4943-aa4c-dea9f6e87665",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f3d08a6-0870-4a60-9d5a-297b56f660c7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
